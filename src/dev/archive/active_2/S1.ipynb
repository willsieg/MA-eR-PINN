{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb12afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------\n",
    "LSTM Training\n",
    "Version: V1.7\n",
    "Modified: 06.11.2024\n",
    "William Siegle\n",
    "---------------------------------------------------------------------\n",
    "notebook can be converted to python script using: \n",
    "(python -m) jupytext --to py FILENAME.ipynb\n",
    "------------------------------------------------------------------'''\n",
    "from pathlib import Path\n",
    "\n",
    "# SETTINGS ---------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    # SYSTEM: ---------------------------------------------------------------------\n",
    "    \"GPU_SELECT\":       0, # {None, 0,1,2,3}, None: CPU only\n",
    "    \"INPUT_LOCATION\":   Path(\"TripSequences\", \"trips_processed_resampled\"),\n",
    "    \"PTH_LOCATION\":     Path(\"src\", \"models\", \"pth\"),\n",
    "\n",
    "    # PREPROCESSING: --------------------------------------------------------------\n",
    "    \"TRAIN_VAL_TEST\":   [0.8, 0.15, 0.05], # [train, val, test]\n",
    "    \"MAX_FILES\":        10000, #None, # None: all files\n",
    "    \n",
    "    # TORCH: ----------------------------------------------------------------------\n",
    "    \"TORCH_SEED\"  :     42,\n",
    "    \"BATCH_SIZE\":       64,   # [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "    \"NUM_EPOCHS\":       80,\n",
    "    \"LEARNING_RATE\":    4e-4,   # 0.001 lr\n",
    "    \"HIDDEN_SIZE\":      200,\n",
    "    \"NUM_LAYERS\":       2,\n",
    "    \"DROPOUT\":          0.5,\n",
    "    \"SEQ_LEN\":          100\n",
    "}\n",
    "\n",
    "for key in CONFIG: globals()[key] = CONFIG[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b221b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  --------------\n",
    "import sys, os\n",
    "global ROOT, DATA_PATH\n",
    "ROOT = Path('../../..').resolve()\n",
    "print(f\"{'-'*60}\\nDirectories:\\n  {ROOT}:\\t\\t\\t{', '.join([_.name for _ in ROOT.glob('*/')])}\")\n",
    "sys.path.append(os.path.abspath(ROOT))\n",
    "from data import get_data_path  # paths set in \"data/__init__.py\"\n",
    "DATA_PATH = get_data_path()\n",
    "print(f\"  {DATA_PATH}:\\t\\t\\t{', '.join([_.name for _ in DATA_PATH.glob('*/')])}\")\n",
    "# ----------------------------------------------------------------------------------------\n",
    "from src.utils.TripDataset import TripDataset\n",
    "from src.utils.train_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d61ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS ---------------------------------------------------------------------\n",
    "global IS_NOTEBOOK\n",
    "IS_NOTEBOOK = False\n",
    "try:    # if running in IPython\n",
    "    shell = get_ipython().__class__.__name__ # type: ignore \n",
    "    #%reset -f -s\n",
    "    %matplotlib inline\n",
    "    from IPython.display import display, HTML, Javascript\n",
    "    from IPython.core.magic import register_cell_magic\n",
    "    from IPython.display import clear_output\n",
    "    @register_cell_magic    # cells can be skipped by using '%%skip' in the first line\n",
    "    def skip(line, cell): return\n",
    "    from tqdm.notebook import tqdm # type: ignore\n",
    "    IS_NOTEBOOK = True\n",
    "    print(f\"{'-'*60}\\nRunning in notebook mode\")\n",
    "except (NameError, ImportError):    # if running in script\n",
    "    from tqdm import tqdm\n",
    "    from tabulate import tabulate\n",
    "    print(f\"{'-'*60}\\nRunning in script mode\")\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------------------------------\n",
    "# IMPORTS ---------------------------------------------------------------------\n",
    "# IMPORTS ---------------------------------------------------------------------\n",
    "import math, time, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.signal import savgol_filter\n",
    "#from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(TORCH_SEED);\n",
    "\n",
    "# METRICS ---------------------------------------------------------------------\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "from pytorch_forecasting.metrics import MASE\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import ExponentialSmoothing\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.dataprocessing import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dd266",
   "metadata": {},
   "source": [
    "___\n",
    "LOCATE DEVICES & SYSTEM FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a99db4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# DEVICE SELECTION ---------------------------------------------------------------------\n",
    "global DEVICE\n",
    "print(f\"{'-'*60}\\nTorch version: \", torch.__version__)\n",
    "if not torch.cuda.is_available() or GPU_SELECT is None:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "else:\n",
    "    DEVICE = torch.device(f\"cuda:{GPU_SELECT}\")\n",
    "print(f\"Using: -->  {str(DEVICE).upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811b51c",
   "metadata": {},
   "source": [
    "___\n",
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, INPUT_LOCATION) # Trip parquet files\n",
    "pth_folder = Path(ROOT, PTH_LOCATION)\n",
    "print(f\"{'-'*60}\\nInput Data:\\t{input_folder}\\nStore model in:\\t{pth_folder}\")\n",
    "\n",
    "# PREPARE TRAIN & TEST SET ---------------------------------------------------\n",
    "all_files = [Path(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "files = all_files[:MAX_FILES]\n",
    "print(f\"{'-'*60}\\nTotal Files: {len(files)}\\n{'-'*60}\")\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_parquet(Path(input_folder, random.choice(files)), engine='fastparquet')\n",
    "all_signals = df.columns\n",
    "assert len(all_signals) == 44\n",
    "\n",
    "# Filter files based on row count\n",
    "filtered_files = []\n",
    "trip_lengths = []\n",
    "for file in files:\n",
    "    trip_rows = pq.read_metadata(file).num_rows\n",
    "    trip_lengths.append(trip_rows)\n",
    "    if trip_rows >= 1200/5:\n",
    "        filtered_files.append(file)\n",
    "\n",
    "files = filtered_files\n",
    "print(f\"Filtered Files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45170e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40073f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT & TARGET SPECIFICATION ---------------------------------------------------\n",
    "# these signals are required for the physical Model calculation:\n",
    "base_signals = [\"signal_time\", \"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \n",
    "\"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "\n",
    "selection_1 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", 'roadgrad_cval_pt', \"vehweight_cval_pt\", \"accelpdlposn_cval\", \"bs_brk_cval\", \"elcomp_pwrcons_cval\",\n",
    "               \"epto_pwr_cval\", \"motortemperature_pti1\", \"powerstagetemperature_pti1\", 'airtempinsd_cval_hvac', 'brktempra_cval', 'selgr_rq_pt']\n",
    "\n",
    "# these signals have to be dropped in order for appropriate training:\n",
    "columns_to_drop = [\"hv_batmomavldischrgen_cval_1\", \"latitude_cval_ippc\", \"longitude_cval_ippc\", \"signal_time\"]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "target_column = \"hv_bat_soc_cval_bms1\"\n",
    "input_columns = selection_1\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# SELECT RUN:\n",
    "# input_columns = input_columns\n",
    "# input_columns = [\"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "# input_columns = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"bs_roadincln_cval\", \"vehweight_cval_pt\"]\n",
    "# input_columns = [i for i in input_columns if i not in base_signals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE NORMALIZATION/SCALING -----------------------------------------------------------------\n",
    "scaler = MaxAbsScaler() \n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SET SPLITTING -----------------------------------------------------------------------\n",
    "# train_subset, test_subset = train_test_split(files, test_size=0.2, random_state=1)\n",
    "train_subset, val_subset, test_subset = random_split(files, TRAIN_VAL_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATALOADERS  ---------------------------------------------------------------\n",
    "# Note: 1. the scaler will be fitted only on the training data set\n",
    "#       2. shuffling is prohibited to maintain the time series order\n",
    "\n",
    "# TRAIN  ------------------------------------------------------------\n",
    "train_dataset = TripDataset(train_subset, input_columns, target_column, scaler, target_scaler, fit=True)\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# VAL ------------------------------------------------------------\n",
    "val_dataset = TripDataset(val_subset, input_columns, target_column, scaler, target_scaler, fit=False)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# TEST ------------------------------------------------------------\n",
    "test_dataset = TripDataset(test_subset, input_columns, target_column, scaler, target_scaler, fit=False)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Print the size of the datasets ----------------------------------\n",
    "print(f\"{'-'*60}\\nTrain size:  {len(train_dataset)}\\t\\t(Files: {len(train_subset)})\")\n",
    "print(f'Val. size:   {len(val_dataset)}\\t\\t(Files: {len(val_subset)})')\n",
    "print(f'Test size:   {len(test_dataset)}\\t\\t(Files: {len(test_subset)}) \\n {\"-\"*60}')\n",
    "if train_dataset.__len__() != sum(len(data) for data in train_dataset.data): print(\"Warning: Train Dataset Length Mismatch\")\n",
    "\n",
    "subset_files = {\"train\":    list(train_loader.dataset.file_list),\n",
    "                \"val\":      list(val_loader.dataset.file_list),\n",
    "                \"test\":     list(test_loader.dataset.file_list)}\n",
    "print([os.path.basename(_) for _ in subset_files[\"train\"][:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3432e19",
   "metadata": {},
   "source": [
    "___\n",
    "NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM NETWORK -----------------------------------------------------------------------\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, device = DEVICE): #, num_classes, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "\n",
    "        self.input_size = input_size    # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.num_layers = num_layers    # number of layers\n",
    "        self.dropout = dropout\n",
    "        #self.num_classes = num_classes  # number of classes\n",
    "        #self.seq_length = seq_length    # sequence length\n",
    "\n",
    "        # LSTM CELL --------------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size,            # The number of expected features in the input x\n",
    "            self.hidden_size,           # The number of features in the hidden state h\n",
    "            self.num_layers,            # Number of recurrent layers for stacked LSTMs. Default: 1\n",
    "            batch_first = True,         # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Default: False\n",
    "            bias = True,                # If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            dropout = self.dropout,     # usually: [0.2 - 0.5] ,introduces a Dropout layer on the outputs of each LSTM layer except the last layer, (dropout probability). Default: 0\n",
    "            bidirectional = False,      # If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size = 0,              # If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "            device = DEVICE) \n",
    "        \n",
    "        # --------------------------------\n",
    "        #self.fc_1 =  nn.Linear(hidden_size, 128)  # fully connected 1\n",
    "        #self.fc = nn.Linear(128, num_classes)     # fully connected last layer\n",
    "        # --------------------------------\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_test =  nn.Linear(hidden_size, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, input, batch_size = None):\n",
    "        '''        \n",
    "        # initial hidden and internal states\n",
    "        # --------------------------------\n",
    "        h_0 = torch.zeros(self.num_layers, input.size(0) if batch_size is None else batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, input.size(0) if batch_size is None else batch_size, self.hidden_size)  \n",
    "        # --------------------------------\n",
    "        out = self.relu(hn.view(-1, self.hidden_size)) # reshaping the data for Dense layer next\n",
    "        out = self.fc_1(out) # first Dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc(out) # Final Output\n",
    "        '''\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        # --------------------------------\n",
    "        # output, (hn, cn) = self.lstm(input, (h_0, c_0)) # lstm with input, hidden, and internal state\n",
    "        # input shape:      (batch_size, seq_length, input_size)\n",
    "        # output shape:     (batch_size, seq_length, hidden_size)\n",
    "        # --------------------------------\n",
    "        out, _ = self.lstm(input)\n",
    "\n",
    "\n",
    "        # ouput layers\n",
    "        # --------------------------------\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_test(out[:, -1, :])  \n",
    "        #out = self.fc_test(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN NETWORK -----------------------------------------------------------------------\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        \n",
    "        # here, our linear layer will have an input of 200, not 100 as before:\n",
    "        self.fc1 = nn.Linear(200,5)  \n",
    "        self.fc2 = nn.Linear(5,10)   \n",
    "        self.fc3 = nn.Linear(10,100) # but the output remains 100\n",
    "        \n",
    "          \n",
    "    def forward(self, x):\n",
    "        # we have to flatten our 20x2x100 to a 20x200:\n",
    "        x = x.view(x.size(0),-1)     # x.size(0) is 20, and -1 is a shortcut for \"figure out the other number for me please!\"\n",
    "        \n",
    "        # the rest proceeds as before:\n",
    "        x = F.relu(self.fc1(x))      \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)              \n",
    "        return x\n",
    "        \n",
    "#net = FCNN()\n",
    "#print(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74919ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# LAYERS --------------------------------\n",
    "input_size = len(input_columns)     # expected features in the input x\n",
    "hidden_size = HIDDEN_SIZE           # features in the hidden state h\n",
    "num_layers = NUM_LAYERS             # recurrent layers for stacked LSTMs. Default: 1\n",
    "num_classes = 1                     # output classes (=1 for regression)\n",
    "\n",
    "# INSTANTIATE MODEL --------------------\n",
    "model = LSTM1(input_size, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)  #, num_classes, seq_length).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33310f0",
   "metadata": {},
   "source": [
    "___\n",
    "TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# OPTIMIZER -----------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE,\n",
    "    weight_decay = 1e-4      # weight decay coefficient (default: 1e-2)\n",
    "    #betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    #eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    ")\n",
    "\n",
    "# LR SCHEDULER -----------------------------\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 2, factor = 0.5, min_lr = 1e-6)\n",
    "\n",
    "# LOSS FUNCTION ----------------------------------------------------------------\n",
    "def loss_fn(model_output, target):\n",
    "    loss = F.mse_loss(model_output, target) # mean-squared error for regression\n",
    "    return loss\n",
    "\n",
    "# or define criterion function:\n",
    "criterion_list = [nn.MSELoss(), nn.L1Loss(), nn.SmoothL1Loss(), nn.HuberLoss(), MASE()]\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# print Model and Optimizer state_dicts\n",
    "print(f\"{'-'*60}\\n\", model, f\"{'-'*60}\\nModel state_dict:\")\n",
    "for param_tensor in model.state_dict(): print(f\"{param_tensor}:\\t {model.state_dict()[param_tensor].size()}\")\n",
    "print(f\"{'-'*60}\\n{optimizer}\\n{'-'*60}\\n{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9e719",
   "metadata": {},
   "source": [
    "___\n",
    "NETWORK TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK TRAINING -----------------------------------------------------------------\n",
    "trained = train_model(\n",
    "    model = model, \n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler, \n",
    "    loss_fn = criterion, \n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader, \n",
    "    num_epochs = NUM_EPOCHS, \n",
    "    device = DEVICE, \n",
    "    is_notebook = IS_NOTEBOOK\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fe51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL  -----------------------------------------------------------------\n",
    "# create unique model name\n",
    "model_name = f'{model.__class__.__name__}_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}'\n",
    "model_destination_path = Path(pth_folder, model_name + \".pth\")\n",
    "\n",
    "# add the splitted data subset information to save dict\n",
    "trained['subset_files'] = subset_files\n",
    "# save the model & print info\n",
    "torch.save(trained, model_destination_path)\n",
    "print(f\"Model saved to:\\t {model_destination_path}\\n{'-'*60}\\nSize: {os.path.getsize(model_destination_path) / 1e6:.2f} MB\\n{'-'*60}\")\n",
    "if os.path.getsize(model_destination_path) > 100 * 1024**2: print(\"--> Warning: The saved model size exceeds 100MB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL AT CHECKPOINT -----------------------------------------------------------------\n",
    "model_destination_path = Path(pth_folder, \"LSTM1_241118_184156.pth\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "checkpoint = torch.load(model_destination_path, weights_only=False, map_location=DEVICE if torch.cuda.is_available() else torch.device('cpu'))\n",
    "for key in [\"model\", \"loss_fn\", \"training_table\", \"train_losses\", \"val_losses\", \"epoch\"]: exec(f\"{key} = checkpoint[key]\")\n",
    "\n",
    "# configure model and optimizer:\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Move the model and its parameters to the CPU\n",
    "try:\n",
    "    if torch.cuda.is_available() and GPU_SELECT is not None:\n",
    "        model.to(DEVICE)\n",
    "    else:\n",
    "        model.to(torch.device(\"cpu\"))\n",
    "except:\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "\n",
    "model.eval(); # set model to evaluation mode for inference\n",
    "print(f\"Model loaded from:\\t {model_destination_path}\\n{'-'*60}\")\n",
    "print(f\"Model: {model.__class__.__name__}\\t\\tParameters on device: {next(model.parameters()).device}\\n{'-'*60}\\n\"\n",
    "        f\"Train/Batch size:\\t{len(train_loader.dataset)} / {train_loader.batch_size}\\n\"\n",
    "        f\"Loss:\\t\\t\\t{loss_fn}\\nOptimizer:\\t\\t{optimizer.__class__.__name__}\\nLR:\\t\\t\\t\"\n",
    "        f\"{optimizer.param_groups[0]['lr']}\\nWeight Decay:\\t\\t{optimizer.param_groups[0]['weight_decay']}\\n{'-'*60}\\n\", model)\n",
    "\n",
    "# get file list of test subset\n",
    "test_files = checkpoint[\"subset_files\"][\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUME TRAINING -----------------------------------------------------------------\n",
    "resume = False\n",
    "if resume: \n",
    "    NUM_EPOCHS += 5 # train for 2 more epochs\n",
    "    trained = train_model(\n",
    "        model = model, \n",
    "        optimizer = optimizer,\n",
    "        scheduler = scheduler, \n",
    "        loss_fn = criterion, \n",
    "        train_loader = train_loader,\n",
    "        val_loader = val_loader, \n",
    "        state = checkpoint,\n",
    "        num_epochs = NUM_EPOCHS, \n",
    "        device = DEVICE, \n",
    "        is_notebook = IS_NOTEBOOK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DataFrame of training metrics:\n",
    "training_df = pd.DataFrame(training_table, columns=[\"Epoch\", \"Iteration\", \"Batch Loss\", \"Train Loss\"])\n",
    "# Extract the 'Train Loss' column and compare with the train_losses list\n",
    "train_loss_column = training_df['Train Loss'].replace(['',' '], np.nan).dropna().astype(float).values\n",
    "if any(abs(train_loss_column - train_losses) > 1e-3): \n",
    "    print(\"Extracted and original Train Losses are not equal. Please check metrics table.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# plot training performance:\n",
    "fig, ax1 = plt.subplots(figsize=(10,4))\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_xticks(range(1, NUM_EPOCHS + 1))\n",
    "\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='train_loss')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), val_losses, label='val_loss')\n",
    "plt.style.use('ggplot'); plt.yscale('log'); fig.tight_layout(); plt.legend();\n",
    "\n",
    "plt.text(0.8, 0.3, f\"Train: {train_losses[-1]:.3e}\\nVal:    {val_losses[-1]:.3e}\", transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c1817",
   "metadata": {},
   "source": [
    "___\n",
    "EVALUATION / POST-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION -----------------------------------------------------------------\n",
    "model.eval() # set model to evaluation mode\n",
    "test_loss = 0\n",
    "all_outputs, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Collect all outputs and targets\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "        all_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Inverse-transform on all outputs and targets for evaluation\n",
    "scaled_outputs = target_scaler.inverse_transform(np.concatenate(all_outputs, axis=0).reshape(-1, 1))\n",
    "scaled_targets = target_scaler.inverse_transform(np.concatenate(all_targets, axis=0).reshape(-1, 1))\n",
    "\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss:  {test_loss:.4f}\")\n",
    "print(f\"Iterations: {iter}/{math.floor(len(test_loader.dataset) / test_loader.batch_size)}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(scaled_targets, scaled_outputs):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(scaled_targets - scaled_outputs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TripDataset(random.sample(test_files, 1), input_columns, target_column, scaler, target_scaler, fit=False)\n",
    "test_loader_2 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_2:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        y_pred.append(torch.mean(outputs).item())\n",
    "\n",
    "y_pred = target_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
    "y_true = target_scaler.inverse_transform(np.array(test_loader_2.dataset.targets[0]).reshape(-1, 1))\n",
    "\n",
    "###############################################\n",
    "# PLOT PREDICTION -----------------------------------------------------------------\n",
    "plt.figure(figsize=(18,4)); plt.xlabel('Time in s'); plt.ylabel('Battery Energy in kWh'); plt.title('Time-Series Prediction')\n",
    "plt.plot(y_true, label='Actual Data') # actual plot\n",
    "plt.plot(np.arange(0, len(y_true), 1), y_pred, label='Predicted Data') # predicted plot\n",
    "plt.legend()\n",
    "plt.text(0.01, 0.02, f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.figure(figsize=(18,4)); plt.xlabel('Time in s'); plt.ylabel('Battery Energy in kWh'); plt.title('Time-Series Prediction (Smoothed)')\n",
    "plt.plot(savgol_filter(y_true.flatten(), window_length=60, polyorder=3), label='Actual Data (Smoothed)') # actual plot\n",
    "plt.plot(np.arange(0, len(y_true), 1), savgol_filter(y_pred.flatten(), window_length=60, polyorder=3), label='Predicted Data (Smoothed)') # predicted plot\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
