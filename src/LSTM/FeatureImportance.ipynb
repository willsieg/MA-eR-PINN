{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# SETTINGS ------------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    # SYSTEM: ---------------------------------------------------------------------\n",
    "    \"ROOT\":             Path('../..').resolve(),\n",
    "    \"INPUT_LOCATION\":   Path(\"TripSequences\", \"trips_processed_final\"), \n",
    "    \"OUTPUT_LOCATION\":  Path(\"src\", \"models\", \"pth\"),\n",
    "    \"GPU_SELECT\":       0, # {0,1,2,3, None: CPU only}\n",
    "    \"TORCH_SEED\"  :     17,\n",
    "\n",
    "    # DATA PREPROCESSING: ---------------------------------------------------------\n",
    "    \"TRAIN_VAL_TEST\":   [1, 0, 0], # [train, val, test splits]\n",
    "    \"MAX_FILES\":        None, # None: all files\n",
    "    \"SCALERS\":          {'feature_scaler': 'MaxAbsScaler()', 'target_scaler': 'MinMaxScaler(feature_range=(0, 1))'},\n",
    "    \"MIN_SEQ_LENGTH\":   60, # minimum sequence length in seconds\n",
    "\n",
    "    # FEATURES: -------------------------------------------------------------------\n",
    "    \"FEATURES\":         [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", 'roadgrad_cval_pt', \"vehweight_cval_pt\", \"accelpdlposn_cval\", \n",
    "                         \"bs_brk_cval\", \"elcomp_pwrcons_cval\",\"epto_pwr_cval\", \"motortemperature_pti1\", \"powerstagetemperature_pti1\", 'airtempinsd_cval_hvac', \n",
    "                         'brktempra_cval', 'selgr_rq_pt'],\n",
    "    \"TARGETS\":          ['hv_bat_soc_cval_bms1'],\n",
    "\n",
    "    # MODEL: -----------------------------------------------------------------------\n",
    "    \"HIDDEN_SIZE\":      400,    # features in the hidden state h\n",
    "    \"NUM_LAYERS\":       2,      # recurrent layers for stacked LSTMs. Default: 1\n",
    "    \"DROPOUT\":          0.5,\n",
    "    \"SEQ_LENGTH\":       60,\n",
    "    \n",
    "    # TRAINING & OPTIMIZER: --------------------------------------------------------\n",
    "    \"NUM_EPOCHS\":       100,\n",
    "    \"BATCH_SIZE\":       32,   # [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "    \"LEARNING_RATE\":    3e-3,   # 0.001 lr\n",
    "    \"OPTIMIZER\":        \"torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = 1e-3)\",      \n",
    "                            # weight_decay = 1e-4     # weight decay coefficient (default: 1e-2)\n",
    "                            # betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "                            # eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"LRSCHEDULER\":      \"torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 2, factor = 0.5, min_lr = 1e-7)\",\n",
    "\n",
    "    # LOSS FUNCTION: ---------------------------------------------------------------\n",
    "    \"CRITERION\":        \"nn.SmoothL1Loss()\", #['nn.MSELoss()', 'nn.L1Loss()', 'nn.SmoothL1Loss()', 'nn.HuberLoss()', 'MASE()']\n",
    "\n",
    "\n",
    "    # METRICS: ---------------------------------------------------------------------\n",
    "\n",
    "    # SAVE & LOAD: -----------------------------------------------------------------\n",
    "\n",
    "}\n",
    "\n",
    "for key in CONFIG: globals()[key] = CONFIG[key]\n",
    "print(f\"ROOT: {ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Directories:\n",
      "  C:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN:\t\t\t.git, archive, data, project, ref, src, test\n",
      "  C:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-Data:\t\t\tnew_folder, OLD, processed, TripSequences\n",
      "------------------------------------------------------------\n",
      "Running in notebook mode\n"
     ]
    }
   ],
   "source": [
    "# LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  ---------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "import sys, os\n",
    "global ROOT, DATA_PATH\n",
    "if 'ROOT' not in globals(): ROOT = Path('../..').resolve()\n",
    "print(f\"{'-'*60}\\nDirectories:\\n  {ROOT}:\\t\\t\\t{', '.join([_.name for _ in ROOT.glob('*/')])}\")\n",
    "sys.path.append(os.path.abspath(ROOT))\n",
    "from data import get_data_path  # paths set in \"data/__init__.py\"\n",
    "DATA_PATH = get_data_path()\n",
    "print(f\"  {DATA_PATH}:\\t\\t\\t{', '.join([_.name for _ in DATA_PATH.glob('*/')])}\")\n",
    "\n",
    "# INTERNAL MODULE IMPORTS ----------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "from src.utils.TripDataset import *\n",
    "from src.utils.train_model import *\n",
    "\n",
    "# NOTEBOOK / SCRIPT SETTINGS -------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "global IS_NOTEBOOK\n",
    "IS_NOTEBOOK = False\n",
    "try:    # if running in IPython\n",
    "    shell = get_ipython().__class__.__name__ # type: ignore \n",
    "    from IPython.display import display, HTML, Javascript, clear_output\n",
    "    from IPython.core.magic import register_cell_magic\n",
    "    @register_cell_magic    # cells can be skipped by using '%%skip' in the first line\n",
    "    def skip(line, cell): return\n",
    "    from tqdm.notebook import tqdm as tqdm_nb\n",
    "    IS_NOTEBOOK = True\n",
    "    print(f\"{'-'*60}\\nRunning in notebook mode\")\n",
    "except (NameError, ImportError):    # if running in script\n",
    "    from tqdm import tqdm as tqdm\n",
    "    from tabulate import tabulate\n",
    "    print(f\"{'-'*60}\\nRunning in script mode\")\n",
    "    \n",
    "# GENERAL MODULE IMPORTS -----------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "import math, time, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import pyarrow.parquet as pq\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "torch.set_default_dtype(torch.float32); torch.manual_seed(TORCH_SEED);\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from pytorch_forecasting.metrics import MASE\n",
    "#from darts import TimeSeries\n",
    "#from darts.models import ExponentialSmoothing\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Torch version:  2.5.1+cpu\n",
      "Using: -->  CPU\n"
     ]
    }
   ],
   "source": [
    "# DEVICE SELECTION ---------------------------------------------------------------------\n",
    "global DEVICE\n",
    "print(f\"{'-'*60}\\nTorch version: \", torch.__version__)\n",
    "if not torch.cuda.is_available() or GPU_SELECT is None:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "else:\n",
    "    DEVICE = torch.device(f\"cuda:{GPU_SELECT}\")\n",
    "print(f\"Using: -->  {str(DEVICE).upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input Data:\tC:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-Data\\TripSequences\\trips_processed_final\n",
      "Store model in:\tC:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN\\src\\models\\pth\n",
      "------------------------------------------------------------\n",
      "Total Files:\t18629\n",
      "Filtered Files:\t18629\n",
      "------------------------------------------------------------\n",
      "                FileName  Length  Index\n",
      "0        V13_T25.parquet   20843   2426\n",
      "1       V18_T775.parquet   19425  15996\n",
      "2       V13_T352.parquet   18308   2540\n",
      "3       V18_T972.parquet   17858  16215\n",
      "4      V16_T1629.parquet   17519   7449\n",
      "...                  ...     ...    ...\n",
      "18624   V13_T127.parquet     112   2292\n",
      "18625   V16_T523.parquet     112   8493\n",
      "18626  V17_T4908.parquet     112  13358\n",
      "18627   V101_T80.parquet     110    727\n",
      "18628    V4_T515.parquet     110  18422\n",
      "\n",
      "[18629 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, INPUT_LOCATION) # Trip parquet files\n",
    "pth_folder = Path(ROOT, OUTPUT_LOCATION)\n",
    "print(f\"{'-'*60}\\nInput Data:\\t{input_folder}\\nStore model in:\\t{pth_folder}\")\n",
    "\n",
    "# PREPARE TRAIN & TEST SET ---------------------------------------------------\n",
    "all_files = [Path(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "files = all_files[:MAX_FILES]\n",
    "print(f\"{'-'*60}\\nTotal Files:\\t{len(files)}\")\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_parquet(Path(input_folder, random.choice(files)), engine='fastparquet')\n",
    "all_signals = df.columns\n",
    "assert len(all_signals) == 44\n",
    "\n",
    "# FILTER INPUT FILES --------------------------------------------------------\n",
    "# generate lengths of all files by reading metadata or using presaved lengths\n",
    "try:\n",
    "    presaved_lengths = pd.read_pickle(Path(ROOT, 'data', 'df_files_lengths.pickle'))\n",
    "    presaved_lengths = presaved_lengths.set_index('FileName').to_dict()['Length']\n",
    "    trip_lengths = [presaved_lengths[file.name] for file in files]\n",
    "except:\n",
    "    print(f\"{'-'*60}\\nObtaining sequence lengths... (may take up to 5 minutes)\")\n",
    "    trip_lengths = [pq.read_metadata(file).num_rows for file in files]\n",
    "\n",
    "# discard all items shorter than min_seq_length\n",
    "min_seq_length = MIN_SEQ_LENGTH\n",
    "filtered_files = []\n",
    "filtered_lengths = []\n",
    "for file, length in zip(files, trip_lengths):\n",
    "    if length > min_seq_length: \n",
    "        filtered_files.append(file)\n",
    "        filtered_lengths.append(length)\n",
    "\n",
    "# replace lists with only filtered items\n",
    "files = filtered_files\n",
    "trip_lengths = filtered_lengths\n",
    "print(f\"Filtered Files:\\t{len(files)}\\n{'-'*60}\")\n",
    "\n",
    "# SORT INPUT FILES BY SEQUENCE LENGTH --------------------------------------\n",
    "# this is needed in order to later sort the sequence by their length\n",
    "file_length_mapping = sorted([(file.name, length, idx) for idx, (file, length) in enumerate(zip(files, trip_lengths))], \\\n",
    "    key=lambda x: x[1], reverse=True)\n",
    "\n",
    "file_length_df = pd.DataFrame(file_length_mapping, columns=['FileName', 'Length', 'Index'])\n",
    "print(file_length_df)\n",
    "\n",
    "indices_by_length = file_length_df['Index'].to_list()\n",
    "sorted_trip_lengths = file_length_df['Length'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT & TARGET SPECIFICATION ---------------------------------------------------\n",
    "# these signals are required for the physical Model calculation:\n",
    "base_signals = [\"signal_time\", \"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \n",
    "\"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "\n",
    "# these signals have to be dropped in order for appropriate training:\n",
    "columns_to_drop = [\"hv_batmomavldischrgen_cval_1\", \"latitude_cval_ippc\", \"longitude_cval_ippc\", \"signal_time\", \"hirestotalvehdist_cval_icuc\"]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "selection_1 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", 'roadgrad_cval_pt', \"vehweight_cval_pt\", \"accelpdlposn_cval\", \"bs_brk_cval\", \"elcomp_pwrcons_cval\",\n",
    "               \"epto_pwr_cval\", \"motortemperature_pti1\", \"powerstagetemperature_pti1\", 'airtempinsd_cval_hvac', 'brktempra_cval', 'selgr_rq_pt']\n",
    "selection_2 = [\"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\", \"roadgrad_cval_pt\"]\n",
    "selection_3 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"vehweight_cval_pt\"]\n",
    "\n",
    "# FEATURE SELECTION  --------------------------------------\n",
    "# ---------------------------------------------------------\n",
    "target_column = TARGETS\n",
    "input_columns = FEATURES\n",
    "input_columns = list(set(all_signals) - set(columns_to_drop)) # include target to generate one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:473: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n",
      "c:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:473: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# FEATURE NORMALIZATION/SCALING -----------------------------------------------------------------\n",
    "scaler = eval(SCALERS['feature_scaler'])\n",
    "target_scaler = eval(SCALERS['target_scaler'])\n",
    "\n",
    "# DATA SET SPLITTING -----------------------------------------------------------------------\n",
    "train_subset, val_subset, test_subset = random_split(files, TRAIN_VAL_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting Scalers: MaxAbsScaler, MinMaxScaler\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# TRAIN  ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     32\u001b[0m train_subset\u001b[38;5;241m.\u001b[39mindices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices_by_length \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(train_subset\u001b[38;5;241m.\u001b[39mindices))]\n\u001b[1;32m---> 33\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTripDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m train_dataset_batches \u001b[38;5;241m=\u001b[39m create_batches(train_dataset, BATCH_SIZE)\n\u001b[0;32m     35\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset_batches, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataloader_settings)\n",
      "File \u001b[1;32m~\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN\\src\\utils\\TripDataset.py:30\u001b[0m, in \u001b[0;36mTripDataset.__init__\u001b[1;34m(self, file_list, input_columns, target_column, scaler, target_scaler, fit)\u001b[0m\n\u001b[0;32m     28\u001b[0m num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list):\n\u001b[1;32m---> 30\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     X \u001b[38;5;241m=\u001b[39m df[input_columns]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     32\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[target_column]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to match the shape of the input: 2D array with one column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1793\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1787\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1359\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1357\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[1;32m-> 1359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileSystemDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfragment\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphysical_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilesystem\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GENERATE DATALOADERS  ---------------------------------------------------------------\n",
    "'''\n",
    "Notes: for each of the three subsets, the following steps are performed:\n",
    "    1. Sort each subset by descending sequence lengths based on the obtained indices\n",
    "    2. Create a (custom) TripDataset object to select the input and target columns and apply the scalers. In case\n",
    "         of the training subset, the scalers will be fitted to the training set first.\n",
    "    3. Create a (custom) BatchDataset object of the corresponding TripDataset to handle the sequence padding before using \n",
    "            the DataLoader to create the batches.\n",
    "    4. The DataLoader will then be used to iterate over the batches during training. To use the integrated collate_fn function\n",
    "            of the DataLoader, the batch_size has to be set to 1. The actual batch size is then handled by the BatchDataset object.\n",
    "    5. The collate_fn that is integrated in the DataLoader will automatically handle the shuffling, padding and packing\n",
    "            of the sequences. The DataLoader will return a tuple of (packed_inputs, padded_targets, lengths), where\n",
    "            the packed_inputs are PackedSequence objects that can be efficiently processed by RNNs.\n",
    "            [Output tuple of types (<class 'torch.nn.utils.rnn.PackedSequence'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>)]\n",
    "\n",
    "Note: shuffling will be done batchwise, however inside each batch the sequences  will remain sorted by length\n",
    "\n",
    "*Note: Because of the BatchDataset object in the train loader, \"batch_size\" refers to the number of batches to feed, not the \n",
    "number of samples in a batch. Also, the \"drop_last\" argument is useless due to this.\n",
    "'''\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "dataloader_settings = {\n",
    "    'batch_size': 1,                # see *Note above\n",
    "    'shuffle': True,                # shuffle the batches before each epoch\n",
    "    'collate_fn': collate_fn,       # include optional arguments\n",
    "    'num_workers': 4,               # number of workers\n",
    "    'pin_memory': False if DEVICE.type == 'cpu' else True\n",
    "}\n",
    "\n",
    "# TRAIN  ------------------------------------------------------------\n",
    "train_subset.indices = [i for i in indices_by_length if i in set(list(train_subset.indices))]\n",
    "train_dataset = TripDataset(train_subset, input_columns, target_column, scaler, target_scaler, fit=True)\n",
    "#train_dataset_batches = create_batches(train_dataset, BATCH_SIZE)\n",
    "#train_loader = DataLoader(train_dataset_batches, **dataloader_settings)\n",
    "\n",
    "'''\n",
    "# VAL --------------------------------------------------------------\n",
    "val_subset.indices = [i for i in indices_by_length if i in set(list(val_subset.indices))]\n",
    "val_dataset = TripDataset(val_subset, input_columns, target_column, scaler, target_scaler, fit=False)\n",
    "val_dataset_batches = create_batches(val_dataset, BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset_batches, **dataloader_settings)\n",
    "\n",
    "# TEST -------------------------------------------------------------\n",
    "test_subset.indices = [i for i in indices_by_length if i in set(list(test_subset.indices))]\n",
    "test_dataset = TripDataset(test_subset, input_columns, target_column, scaler, target_scaler, fit=False)\n",
    "test_dataset_batches = create_batches(test_dataset, BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset_batches, **dataloader_settings)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Concatenate all data from the train dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m all_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Concatenate all data from the train dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m all_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN\\src\\utils\\TripDataset.py:72\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     71\u001b[0m     index \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_lengths[file_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[file_idx][index]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),  \u001b[38;5;66;03m# Add time dimension\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[file_idx][index]\n\u001b[0;32m     75\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\SIEGLEW\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3598\u001b[0m, in \u001b[0;36mInteractiveShell.run_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   3597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3598\u001b[0m         result\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshowtraceback(running_compiled_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3600\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len(train_dataset.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data from the train dataset\n",
    "all_data = pd.concat([train_dataset[i][0].cpu().numpy() for i in range(len(train_dataset))], axis=0)\n",
    "# Convert to DataFrame\n",
    "all_data_df = pd.DataFrame(all_data, columns=input_columns + target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to a single parquet file\n",
    "output_file = Path(pth_folder, \"all_data.parquet\")\n",
    "all_data_df.to_parquet(output_file, engine='pyarrow', compression='snappy')\n",
    "\n",
    "print(f\"All data saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
