{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cfaf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f -s\n",
    "%matplotlib inline\n",
    "'''------------------------------------------------------------------\n",
    "MA-eR-PINN: eRange Prediction using Physics-Informed Neural Networks\n",
    "---------------------------------------------------------------------\n",
    "Version: V2.1      Modified: 15.01.2025        William Siegle\n",
    "---------------------------------------------------------------------\n",
    "PTRAIN - Standard Pipeline Framework for Training the PINN\n",
    "+ OPTUNA - Hyperparameter Optimization using Optuna\n",
    "------------------------------------------------------------------''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55581895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA-eR-PINN: CONFIGURATION FILE -------------------------------------------------\n",
    "from pathlib import Path\n",
    "CONFIG = {\n",
    "    # SYSTEM: ---------------------------------------------------------------------\n",
    "    \"GPU_SELECT\":       0,\n",
    "    \"ROOT\":             Path('../..').resolve(),\n",
    "    \"INPUT_LOCATION\":   Path(\"TripSequences\", \"trips_processed_pinn_4\"), \n",
    "    \"OUTPUT_LOCATION\":  Path(\"src\", \"models\", \"pth\"),\n",
    "    \"SEED\"  :           14,\n",
    "    \"MIXED_PRECISION\":  True,\n",
    "    \"EARLY_STOPPING\":   False,\n",
    "\n",
    "    # DATA PREPROCESSING: ---------------------------------------------------------\n",
    "    \"TRAIN_VAL_TEST\":   [0.7, 0.15, 0.15], # [train, val, test splits]\n",
    "    \"MAX_FILES\":        None, # None: all files\n",
    "    \"MIN_SEQ_LENGTH\":   600, # minimum sequence length in s to be included in DataSets\n",
    "    \"SCALERS\":          {'feature_scaler': 'MinMaxScaler()','target_scaler': 'MinMaxScaler()','prior_scaler': 'MinMaxScaler()'},\n",
    "\n",
    "    # FEATURES: -------------------------------------------------------------------\n",
    "    \"FEATURES\":         ['accelpdlposn_cval','actdrvtrnpwrprc_cval','actualdcvoltage_pti1','actualspeed_pti1','actualtorque_pti1',\n",
    "                        'airtempinsd_cval_hvac','airtempinsd_rq','airtempoutsd_cval_cpc','altitude_cval_ippc','brc_stat_brc1','brktempra_cval',\n",
    "                        'bs_brk_cval','currpwr_contendrnbrkresist_cval','elcomp_pwrcons_cval','epto_pwr_cval','hv_bat_dc_momvolt_cval_bms1',\n",
    "                        'hv_batavcelltemp_cval_bms1','hv_batcurr_cval_bms1','hv_batisores_cval_e2e','hv_batmaxchrgpwrlim_cval_1',\n",
    "                        'hv_batmaxdischrgpwrlim_cval_1','hv_curr_cval_dcl1','hv_dclink_volt_cval_dcl1','hv_ptc_cabin1_pwr_cval','hv_pwr_cval_dcl1',\n",
    "                        'lv_convpwr_cval_dcl1','maxrecuppwrprc_cval','maxtracpwrpct_cval','motortemperature_pti1','powerstagetemperature_pti1',\n",
    "                        'rmsmotorcurrent_pti1','roadgrad_cval_pt','selgr_rq_pt','start_soc','txoiltemp_cval_tcm','vehspd_cval_cpc','vehweight_cval_pt'],\n",
    "    \"TARGETS\":          ['hv_bat_soc_cval_bms1'],\n",
    "    \"PRIORS\":           ['emot_soc_pred'],  \n",
    "\n",
    "    # MODEL: -----------------------------------------------------------------------\n",
    "    \"HIDDEN_SIZE\":      100,    # features in the hidden state h\n",
    "    \"NUM_LAYERS\":       4,      # recurrent layers for stacked LSTMs. Default: 1\n",
    "    \"DROPOUT\":          0.05,  # usually: [0.2 - 0.5]\n",
    "    \n",
    "    # TRAINING & OPTIMIZER: --------------------------------------------------------\n",
    "    \"NUM_EPOCHS\":       50,         # max epochs\n",
    "    \"BATCH_SIZE\":       256,         # [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    \"LEARNING_RATE\":    0.0003,     # 0.001 lr\n",
    "    \"OPTIMIZER\":        'adam',     # ('adam', 'sgd', 'adamw')\n",
    "    \"WEIGHT_DECAY\":     1e-7,       # weight decay coefficient (default: 1e-2)\n",
    "    \"WEIGHT_INIT_TYPE\": 'he',       # ('he', 'normal', 'default')\n",
    "    \"CLIP_GRAD\":        None,       # default: None\n",
    "    \"LRSCHEDULER\":      \"torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\",  # constant LR for 1.0 as multiplicative factor\n",
    "\n",
    "    # LOSS FUNCTION: ---------------------------------------------------------------\n",
    "    \"LPSCHEDULER\":      \"ParameterScheduler(initial_value=1.0, schedule_type='reverse_sigmoid', total_epochs=NUM_EPOCHS)\",\n",
    "    #       'constant': [], 'time_based': ['decay_rate'], 'step_based': ['drop_rate', 'epochs_drop'], 'exponential': ['decay_rate'], 'linear': ['absolute_reduction'], \n",
    "    #       'cosine_annealing': ['total_epochs'], 'cyclic': ['base_lr', 'max_lr', 'step_size'], 'reverse_sigmoid': ['total_epochs']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222d1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION MODULES ----------------------------------------------------------------  \n",
    "from torch import nn\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "class CustomLoss(nn.Module):        # (1-l)*L_MSE + l*L_phys\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true, y_phys, l_p):\n",
    "        mse_loss_value = self.mse_loss(y_pred, y_true)                      # loss w.r.t. data\n",
    "        phys_loss_value = self.mse_loss(y_pred, y_phys)                     # loss w.r.t. physical model\n",
    "        total_loss = (1 - l_p) * mse_loss_value + l_p * phys_loss_value     # total loss, weighted by l_p-factor\n",
    "        loss_components = (mse_loss_value, phys_loss_value)\n",
    "        return total_loss, loss_components\n",
    "\n",
    "global LOSS_FN\n",
    "LOSS_FN = CustomLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c4680",
   "metadata": {},
   "source": [
    "___\n",
    "SETUP: Locate devices & system folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7924eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sieglew/MA-eR-PINN\n",
      "------------------------------------------------------------\n",
      "Directories:\n",
      "  /home/sieglew/MA-eR-PINN:\t\t\ttest, .git, archive, project, data, src\n",
      "  /mnt/nvme/datasets/sieglew:\t\t\tTripSequences\n",
      "------------------------------------------------------------\n",
      "Running in notebook mode\n",
      "CONFIG Dictionary:\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "     Parameter         Value\n",
      "--  ----------------  ---------------------------------------------------------------------------------------------------------\n",
      "0   GPU_SELECT        0\n",
      "1   ROOT              /home/sieglew/MA-eR-PINN\n",
      "2   INPUT_LOCATION    TripSequences/trips_processed_pinn_4\n",
      "3   OUTPUT_LOCATION   src/models/pth\n",
      "4   SEED              14\n",
      "5   MIXED_PRECISION   True\n",
      "6   EARLY_STOPPING    False\n",
      "7   TRAIN_VAL_TEST    [0.7, 0.15, 0.15]\n",
      "8   MAX_FILES         None\n",
      "9   MIN_SEQ_LENGTH    600\n",
      "10  SCALERS           {'feature_scaler': 'MinMaxScaler()', 'target_scaler': 'MinMaxScaler()', 'prior_scaler': 'MinMaxScaler()'}\n",
      "11  FEATURES          ['accelpdlposn_cval',\n",
      "                       'actdrvtrnpwrprc_cval',\n",
      "                       'actualdcvoltage_pti1',\n",
      "                       'actualspeed_pti1',\n",
      "                       'actualtorque_pti1',\n",
      "                       'airtempinsd_cval_hvac',\n",
      "                       'airtempinsd_rq',\n",
      "                       'airtempoutsd_cval_cpc',\n",
      "                       'altitude_cval_ippc',\n",
      "                       'brc_stat_brc1',\n",
      "                       'brktempra_cval',\n",
      "                       'bs_brk_cval',\n",
      "                       'currpwr_contendrnbrkresist_cval',\n",
      "                       'elcomp_pwrcons_cval',\n",
      "                       'epto_pwr_cval',\n",
      "                       'hv_bat_dc_momvolt_cval_bms1',\n",
      "                       'hv_batavcelltemp_cval_bms1',\n",
      "                       'hv_batcurr_cval_bms1',\n",
      "                       'hv_batisores_cval_e2e',\n",
      "                       'hv_batmaxchrgpwrlim_cval_1',\n",
      "                       'hv_batmaxdischrgpwrlim_cval_1',\n",
      "                       'hv_curr_cval_dcl1',\n",
      "                       'hv_dclink_volt_cval_dcl1',\n",
      "                       'hv_ptc_cabin1_pwr_cval',\n",
      "                       'hv_pwr_cval_dcl1',\n",
      "                       'lv_convpwr_cval_dcl1',\n",
      "                       'maxrecuppwrprc_cval',\n",
      "                       'maxtracpwrpct_cval',\n",
      "                       'motortemperature_pti1',\n",
      "                       'powerstagetemperature_pti1',\n",
      "                       'rmsmotorcurrent_pti1',\n",
      "                       'roadgrad_cval_pt',\n",
      "                       'selgr_rq_pt',\n",
      "                       'start_soc',\n",
      "                       'txoiltemp_cval_tcm',\n",
      "                       'vehspd_cval_cpc',\n",
      "                       'vehweight_cval_pt']\n",
      "12  TARGETS           ['hv_bat_soc_cval_bms1']\n",
      "13  PRIORS            ['emot_soc_pred']\n",
      "14  HIDDEN_SIZE       100\n",
      "15  NUM_LAYERS        4\n",
      "16  DROPOUT           0.05\n",
      "17  NUM_EPOCHS        50\n",
      "18  BATCH_SIZE        256\n",
      "19  LEARNING_RATE     0.0003\n",
      "20  OPTIMIZER         adam\n",
      "21  WEIGHT_DECAY      1e-07\n",
      "22  WEIGHT_INIT_TYPE  he\n",
      "23  CLIP_GRAD         None\n",
      "24  LRSCHEDULER       torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
      "25  LPSCHEDULER       ParameterScheduler(initial_value=1.0, schedule_type='reverse_sigmoid', total_epochs=NUM_EPOCHS) \n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Torch version:  2.5.1+cu124\n",
      "Using: -->  CUDA:0\n",
      "Timestamp: 250131_162145\n",
      "------------------------------------------------------------\n",
      "Total Files:\t6626\n",
      "Filtered Files:\t6626\n",
      "------------------------------------------------------------\n",
      "               FileName  Length  Index\n",
      "0       V13_T25.parquet   20843   3868\n",
      "1      V13_T352.parquet   18308   3630\n",
      "2     V18_T1224.parquet   16645   5939\n",
      "3      V101_T37.parquet   16591   4718\n",
      "4      V13_T486.parquet   16434   3811\n",
      "...                 ...     ...    ...\n",
      "6621  V17_T4757.parquet     645   2031\n",
      "6622  V14_T1967.parquet     643   5314\n",
      "6623    V4_T260.parquet     636   4530\n",
      "6624    V4_T244.parquet     628   3798\n",
      "6625  V14_T2036.parquet     625   3635\n",
      "\n",
      "[6626 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  ---------------------------\n",
    "import sys, os\n",
    "for key in CONFIG: globals()[key] = CONFIG[key]\n",
    "if 'ROOT' not in globals(): ROOT = Path('../..').resolve()\n",
    "sys.path.append(os.path.abspath(ROOT)); print(ROOT)\n",
    "\n",
    "# INTERNAL MODULE IMPORTS ----------------------------------------------------------------\n",
    "from src.__init__ import *\n",
    "from src.utils.data_utils import *\n",
    "from src.utils.preprocess_utils import *\n",
    "from src.utils.eval_utils import *\n",
    "from src.utils.Trainers import *\n",
    "from src.utils.scheduler_utils import *\n",
    "from src.LSTM.DeepLSTM.model_definitions import print_info\n",
    "\n",
    "# SETUP ENVIRONMENT ---------------------------------------------------------------------\n",
    "DATA_PATH, IS_NOTEBOOK, DEVICE, LOG_FILE_NAME, TS = setup_environment(CONFIG, ROOT, SEED, GPU_SELECT)\n",
    "if not IS_NOTEBOOK: output_file = open(f\"{LOG_FILE_NAME}\", \"w\"); sys.stdout = Tee(sys.stdout, output_file); sys.stderr = Tee(sys.stderr, output_file)\n",
    "\n",
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, INPUT_LOCATION) # Trip parquet files\n",
    "pth_folder = Path(ROOT, OUTPUT_LOCATION, f\"{TS}\")\n",
    "pth_folder.mkdir(parents=True, exist_ok=True)\n",
    "files, trip_lengths, indices_by_length, sorted_trip_lengths, all_signals, file_length_df = prepare_data(input_folder, pth_folder, MAX_FILES, MIN_SEQ_LENGTH, ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1963d1",
   "metadata": {},
   "source": [
    "___\n",
    "DATA SELECTION & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb0b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input Signals:\t37\n",
      "Target Signals:\t1\n",
      "Physical Prior Signals:\t1\n",
      "------------------------------------------------------------\n",
      " --> Warning: Removed the last 96 samples to ensure a balanced batch size\n",
      "fitting Scalers: MinMaxScaler, MinMaxScaler, MinMaxScaler\n",
      "\t50% of the fitting done...\n",
      "Done. Create DataSets and DataLoaders...\n",
      "\tNumber of batches created: 17\n",
      "\tNumber of batches created: 6\n",
      "\tNumber of batches created: 4\n",
      "------------------------------------------------------------\n",
      "Train size:  19055759\t\t(Files: 4352)\n",
      "Val. size:   9013278\t\t(Files: 1392)\n",
      "Test size:   3317724\t\t(Files: 786) \n",
      " ------------------------------------------------------------\n",
      "\tRemoved 96 file from the dataset\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# FEATURE SELECTION & SCALING ----------------------------------------------------------------------------\n",
    "INPUT_COLUMNS = FEATURES; TARGET_COLUMN = TARGETS; PRIOR_COLUMN = PRIORS\n",
    "print(f\"{'-'*60}\\nInput Signals:\\t{len(FEATURES)}\\nTarget Signals:\\t{len(TARGETS)}\\nPhysical Prior Signals:\\t{len(PRIORS)}\\n{'-'*60}\")\n",
    "scaler, target_scaler, prior_scaler = eval(SCALERS['feature_scaler']), eval(SCALERS['target_scaler']), eval(SCALERS['prior_scaler'])\n",
    "\n",
    "# DATA SET SPLITTING AND SORTING ----------------------------------------------------------------\n",
    "# train_subset, val_subset, test_subset = random_split(files, TRAIN_VAL_TEST)\n",
    "\n",
    "\n",
    "train_test_indices = file_length_df.index[(file_length_df['Lat'] > 45) & (file_length_df['Lat'] < 60)].tolist()\n",
    "val_indices = file_length_df.index[(file_length_df['Lat'] <= 45) | (file_length_df['Lat'] >= 60)].tolist()\n",
    "\n",
    "random.shuffle(train_test_indices)\n",
    "random.shuffle(val_indices)\n",
    "\n",
    "val_subset = Subset(files, val_indices)                     # All of ESP/SWE Trips for Val\n",
    "cutoff = int(len(train_test_indices) * 0.85)\n",
    "train_subset = Subset(files, train_test_indices[:cutoff])   # 85% of GER Trips for Train\n",
    "test_subset = Subset(files, train_test_indices[cutoff:])    # 15% of GER Trips for Test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATALOADER SETTINGS ------------------------------------------------------------------\n",
    "dataloader_settings = {'batch_size': 1, 'shuffle': True, 'collate_fn': collate_fn_PINN, 'num_workers': 8,\n",
    " 'prefetch_factor': 4, 'persistent_workers': True, 'pin_memory': False if DEVICE.type == 'cpu' else True}\n",
    "\n",
    "# PREPARE TRAIN, VAL & TEST DATALOADERS  ------------------------------------------------------------\n",
    "train_subset, train_dataset, train_dataset_batches, train_loader = prepare_dataloader_PINN(train_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, fit=True, drop_last=True)\n",
    "\n",
    "val_subset, val_dataset, val_dataset_batches, val_loader = prepare_dataloader_PINN(val_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, drop_last=False)\n",
    "\n",
    "test_subset, test_dataset, test_dataset_batches, test_loader = prepare_dataloader_PINN(test_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, drop_last=False)\n",
    "\n",
    "# print dataset info\n",
    "subset_files = print_dataset_sizes(train_dataset, val_dataset, test_dataset, train_subset, val_subset, test_subset, files)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Load dataloaders instead\n",
    "#train_loader = torch.load('train_loader.pth')\n",
    "#val_loader = torch.load('val_loader.pth')\n",
    "#test_loader = torch.load('test_loader.pth')\n",
    "\n",
    "# optional visualizations of padding preprocessing:\n",
    "if IS_NOTEBOOK and False: \n",
    "    check_batch_PINN(train_loader)\n",
    "    visualize_padding(BATCH_SIZE, trip_lengths, sorted_trip_lengths, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3fe74",
   "metadata": {},
   "source": [
    "___\n",
    "MODEL & TRAINING CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ef486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM NETWORK -----------------------------------------------------------------------\n",
    "class DeepLSTM_v3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, device=DEVICE):\n",
    "        super(DeepLSTM_v3, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, device=device)  # LSTM Dropout = 0 !\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc4 = nn.Linear(hidden_size // 2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, packed_input, batch_size=None):\n",
    "        packed_out, _ = self.lstm(packed_input)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.bn3(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "    # Define the weight initialization function for LSTM and other layers\n",
    "    def initialize_weights_lstm(self, init_type):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name or 'weight_hh' in name:\n",
    "                if init_type == 'he': nn.init.kaiming_uniform_(param.data, nonlinearity='relu')  # HE INIT\n",
    "                elif init_type == 'normal': nn.init.normal_(param.data, mean=0.0, std=0.02)  # NORMAL INIT\n",
    "                elif init_type == 'default': continue  # TORCH DEFAULT INIT\n",
    "            elif 'weight' in name:\n",
    "                if param.dim() >= 2:  # Ensure the tensor has at least 2 dimensions\n",
    "                    if init_type == 'he': nn.init.kaiming_uniform_(param.data, nonlinearity='relu')  # HE INIT for FC layers\n",
    "                    elif init_type == 'normal': nn.init.normal_(param.data, mean=0.0, std=0.02)  # NORMAL INIT for FC layers\n",
    "                    elif init_type == 'default': continue  # TORCH DEFAULT INIT\n",
    "            elif 'bias' in name and init_type != 'default': nn.init.constant_(param.data, 0)  # Initialize biases to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0d8b3",
   "metadata": {},
   "source": [
    "___\n",
    "PREPARE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef9c4426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " DeepLSTM_v3(\n",
      "  (lstm): LSTM(37, 100, num_layers=4, batch_first=True, dropout=0.05)\n",
      "  (dropout_layer): Dropout(p=0.05, inplace=False)\n",
      "  (fc1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (bn3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ") ------------------------------------------------------------\n",
      "Model state_dict:\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATE MODEL --------------------\n",
    "model = DeepLSTM_v3(len(INPUT_COLUMNS), HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "model.initialize_weights_lstm(WEIGHT_INIT_TYPE); print_info(model)\n",
    "\n",
    "# SET OPTIMIIZER, SCHEDULER AND LOSS MODULES ---------------------------\n",
    "if OPTIMIZER=='adam': optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "elif OPTIMIZER=='adamw': optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "elif OPTIMIZER=='sgd': optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM_SGD, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def lr_lambda(epoch): return 1.0\n",
    "lr_scheduler = eval(LRSCHEDULER); l_p_scheduler = eval(LPSCHEDULER)\n",
    "\n",
    "# SET TRAINER -----------------------------------------------------------------\n",
    "TRAINER = PTrainer_PINN(\n",
    "    model = model, \n",
    "    optimizer = optimizer, \n",
    "    lr_scheduler = lr_scheduler,\n",
    "    l_p_scheduler = l_p_scheduler,\n",
    "    loss_fn = LOSS_FN, \n",
    "    train_loader = train_loader, \n",
    "    val_loader = val_loader, \n",
    "    test_loader = test_loader, \n",
    "    num_epochs = NUM_EPOCHS, \n",
    "    device = DEVICE, \n",
    "    is_notebook = IS_NOTEBOOK, \n",
    "    use_mixed_precision = MIXED_PRECISION, \n",
    "    use_early_stopping = EARLY_STOPPING, \n",
    "    clip_value = CLIP_GRAD, \n",
    "    log_file = Path(LOG_FILE_NAME).with_name(f\"{TS}_log.txt\"),\n",
    "    config = CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d2a72",
   "metadata": {},
   "source": [
    "___\n",
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = TRAINER.train_model()\n",
    "plot_training_performance(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc2065",
   "metadata": {},
   "source": [
    "___\n",
    "SAVE CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7542bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL -----------------------------------------------------------------\n",
    "CHECKPOINT, model_destination_path = save_checkpoint(TRAINER, train_loader, val_loader, test_loader, RESULTS, CONFIG, subset_files, pth_folder, TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cddb8e",
   "metadata": {},
   "source": [
    "___\n",
    "LOAD CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f81daa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from:\t/home/sieglew/MA-eR-PINN/src/models/pth/250130_230541/DeepLSTM_v3_250130_230541.pt\n",
      "------------------------------------------------------------\n",
      "Model: DeepLSTM_v3\tParameters on device: cuda:0\n",
      "------------------------------------------------------------\n",
      "Train/Batch size:\t22 / 200\n",
      "Loss:\t\t\tCustomLoss(\n",
      "  (mse_loss): MSELoss()\n",
      ")\n",
      "Optimizer:\t\tAdam\n",
      "LR:\t\t\t0.0003\n",
      "Weight Decay:\t\t1e-07\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLSTM_v3(\n",
       "  (lstm): LSTM(37, 100, num_layers=4, batch_first=True, dropout=0.05)\n",
       "  (dropout_layer): Dropout(p=0.05, inplace=False)\n",
       "  (fc1): Linear(in_features=100, out_features=200, bias=True)\n",
       "  (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (bn3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_destination_path = Path(pth_folder.parent, \"250130_230541/DeepLSTM_v3_250130_230541.pt\")\n",
    "\n",
    "# LSTM Only, Offline: \"250126_150336/DeepLSTM_v3_250126_150336.pt\"\n",
    "# LSTM + PINN, v1, sigmoid, Offline: \"250126_090748/DeepLSTM_v3_250126_090748.pt\", \"250130_084157/DeepLSTM_v3_250130_084157.pt\"\n",
    "# LSTM + PINN, v1, linear, Offline: \"250129_095318/DeepLSTM_v3_250129_095318.pt\"\n",
    "# LSTM + PINN, v1, exponential, Offline: \"250128_105201/DeepLSTM_v3_250128_105201.pt\"\n",
    "\n",
    "# LSTM + PINN, v2, sigmoid, Offline: \"250125_153204/DeepLSTM_v3_250125_153204.pt\"\n",
    "# LSTM + PINN, v3, sigmoid, Offline: \"250127_224710/DeepLSTM_v3_250127_224710.pt\"\n",
    "\n",
    "# LSTM Only, Online: \"250127_202431/DeepLSTM_v3_250127_202431.pt\"\n",
    "# LSTM + PINN, v1, sigmoid, Online: \"250127_153406/DeepLSTM_v3_250127_153406.pt\"\n",
    "# LSTM + PINN, v1, exponential, Online: \"250127_181242/DeepLSTM_v3_250127_181242.pt\"\n",
    "\n",
    "# LSTM Only, Train/Test GER, Val ESP/SWE: \"250130_230524/DeepLSTM_v3_250130_230524.pt\"\n",
    "# LSTM + PINN, Train/Test GER, Val ESP/SWE: \"250130_230541/DeepLSTM_v3_250130_230541.pt\"\n",
    "\n",
    "\n",
    "# LOAD MODEL -----------------------------------------------------------------\n",
    "CHECKPOINT = load_checkpoint(model_destination_path, DEVICE)\n",
    "for key in CHECKPOINT.keys(): globals()[key] = CHECKPOINT[key]\n",
    "# load model and optimizer states --------------------------------------------\n",
    "model.load_state_dict(model_state_dict)\n",
    "optimizer.load_state_dict(optimizer_state_dict)\n",
    "model.eval()  # set model to evaluation mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [path.name for path in CHECKPOINT[\"test_files\"]]\n",
    "test_indices = [file_length_df.index[file_length_df['FileName'] == filename].tolist()[0] if filename in file_length_df['FileName'].values else -1 for filename in test_files]\n",
    "recreated_test_dataset = Subset(files, test_indices)\n",
    "test_subset, test_dataset, test_dataset_batches, test_loader = prepare_dataloader_PINN(recreated_test_dataset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c5df231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN -----------------------------------------------------------------\n",
    "TRAINER = PTrainer_PINN(\n",
    "    model = model, \n",
    "    optimizer = optimizer, \n",
    "    lr_scheduler = lr_scheduler,\n",
    "    l_p_scheduler = l_p_scheduler,\n",
    "    loss_fn = LOSS_FN, \n",
    "    train_loader = train_loader, \n",
    "    val_loader = val_loader, \n",
    "    test_loader = test_loader, \n",
    "    num_epochs = NUM_EPOCHS, \n",
    "    device = DEVICE, \n",
    "    is_notebook = IS_NOTEBOOK, \n",
    "    use_mixed_precision = MIXED_PRECISION, \n",
    "    use_early_stopping = EARLY_STOPPING, \n",
    "    clip_value = CLIP_GRAD, \n",
    "    log_file = Path(LOG_FILE_NAME).with_name(f\"{TS}_log.txt\"),\n",
    "    config = CONFIG,\n",
    "    state = CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_performance(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d64ba",
   "metadata": {},
   "source": [
    "___\n",
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f09213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION -----------------------------------------------------------------\n",
    "# get file list of test subset\n",
    "test_files = CHECKPOINT[\"test_files\"]; print(f\"{'-'*60}\\nTest subset: {len(test_files)} files\\n{'-'*60}\")\n",
    "# -------------------------------------\n",
    "# evaluate model on test set\n",
    "test_loss, all_outputs, all_targets, all_priors, all_original_lengths = TRAINER.evaluate_model()\n",
    "# -------------------------------------\n",
    "# Inverse-transform on all outputs and targets for evaluation\n",
    "scaled_outputs = [target_scaler.inverse_transform(output_sequence.reshape(1, -1)).squeeze() for output_sequence in all_outputs]\n",
    "scaled_targets = [target_scaler.inverse_transform(target_sequence.reshape(1, -1)).squeeze() for target_sequence in all_targets]\n",
    "scaled_priors = [prior_scaler.inverse_transform(prior_sequence.reshape(1, -1)).squeeze() for prior_sequence in all_priors]\n",
    "\n",
    "# concatenate:\n",
    "all_y_true, all_y_pred, all_y_phys = np.concatenate(scaled_targets), np.concatenate(scaled_outputs), np.concatenate(scaled_priors)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "print(f\"Test Loss:\\t\\t{test_loss:.6f}\")\n",
    "metrics = calculate_metrics(all_y_true, all_y_pred) # [rmse, mae, std_dev, mape, r2, max_error]\n",
    "print(\"Metrics (unweighted):\")\n",
    "mean_metrics = calculate_metrics_per_sequence(scaled_targets, scaled_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ed8d9",
   "metadata": {},
   "source": [
    "___\n",
    "PLOT RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3479df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sample sequence from test set\n",
    "#sample_int = random.randint(1, len(test_files)-1)\n",
    "sample_int += 1\n",
    "y_true, y_pred, y_phys = scaled_targets[sample_int], scaled_outputs[sample_int], scaled_priors[sample_int]\n",
    "print(len(y_true))\n",
    "\n",
    "# ----------->  add specific file selector for easier comparison\n",
    "###############################################\n",
    "# PLOT PREDICTION -----------------------------------------------------------------\n",
    "# -----------\n",
    "fig = plt.figure(figsize=(16,4)); plt.xlabel('Time in s', fontsize=14); plt.ylabel('SOC in %', fontsize=14); plt.title('Battery State of Charge: Prediction vs. Actual Data') \n",
    "plt.plot(y_true, label='Actual Data', linewidth=2)                                             # actual plot\n",
    "plt.plot(y_phys, label='Physical Prior', linewidth=2, linestyle='--')                          # physical prior\n",
    "plt.plot(np.arange(0, len(y_true), 1), y_pred, label='Predicted Data', linewidth=2)            # predicted plot\n",
    "plt.ylim(0, 100); ax = plt.gca(); ax.set_xlim(left=-len(y_true)/100)\n",
    "plt.tight_layout(pad=0.8); fig.patch.set_facecolor('white'); fig.set_facecolor('white'); plt.legend(); plt.grid(True)\n",
    "plt.text(0.007, 0.05, f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\\nStd Dev: {np.std(y_true - y_pred):.4f}\\nModel ID: {model_name_id}\",\\\n",
    "     transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "# -----------\n",
    "fig = plt.figure(figsize=(16,4)); plt.xlabel('Time in s', fontsize=14); plt.ylabel('SOC in %', fontsize=14)\n",
    "plt.plot(savgol_filter(y_true.flatten(), window_length=60, polyorder=3), label='Actual Data (Smoothed)', linewidth=2)   # actual plot\n",
    "plt.plot(np.arange(0, len(y_true), 1), savgol_filter(y_pred.flatten(), window_length=300, polyorder=3), label='Predicted Data (Smoothed)', linewidth=2, color='r')   # predicted plot\n",
    "plt.ylim(0, 100); ax = plt.gca(); ax.set_xlim(left=-len(y_true)/100)\n",
    "plt.tight_layout(pad=0.8); fig.patch.set_facecolor('white'); fig.set_facecolor('white'); plt.legend(); plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e2ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_errors = [np.abs(output - target) for output, target in zip(scaled_outputs, scaled_targets)]\n",
    "\n",
    "# Normalize the time axis for each sequence to a percentage from 0% to 100%\n",
    "def normalize_time_axis(sequence):\n",
    "    return np.linspace(0, 100, len(sequence))\n",
    "\n",
    "normalized_time_axes = [normalize_time_axis(seq) for seq in scaled_outputs]\n",
    "\n",
    "# Interpolate the errors to a common set of percentage points\n",
    "common_percentage_points = np.linspace(0, 100, 100)\n",
    "interpolated_errors = [np.interp(common_percentage_points, time_axis, error) for time_axis, error in zip(normalized_time_axes, absolute_errors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34c3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean error at each percentage point\n",
    "mean_error = np.mean(interpolated_errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "784f7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error_2 = np.mean(interpolated_errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the mean error over the percentage of elapsed time\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "plt.plot(common_percentage_points, mean_error, label='Mean Absolute Error (PINN)', linewidth=2)\n",
    "#plt.plot(common_percentage_points, mean_error_2, label='Mean Absolute Error (LSTM)', linewidth=2)\n",
    "plt.xlabel('Normalized Trip Length in %')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Mean Absolute Error Over Percentage of Elapsed Time')\n",
    "plt.tight_layout(pad=0.8); fig.patch.set_facecolor('white'); fig.set_facecolor('white'); plt.legend(); plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "941a2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE:\n",
    "save_as = 'test_output.pkl'\n",
    "output_data = {'scaled_outputs': scaled_outputs, 'scaled_targets': scaled_targets, 'scaled_priors': scaled_priors}\n",
    "with open(Path(model_destination_path.parent, save_as), 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23767f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD:\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open(Path(model_destination_path.parent, 'test_output.pkl'), 'rb') as f:\n",
    "    output_data = pickle.load(f)\n",
    "\n",
    "# Access the DataFrames\n",
    "scaled_outputs = output_data['scaled_outputs']\n",
    "scaled_targets = output_data['scaled_targets']\n",
    "scaled_priors = output_data['scaled_priors']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sieglew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
