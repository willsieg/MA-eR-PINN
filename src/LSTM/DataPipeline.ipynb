{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN\n"
     ]
    }
   ],
   "source": [
    "'''------------------------------------------------------------------\n",
    "---------------------------------------------------------------------\n",
    "LSTM Training\n",
    "---------------------------------------------------------------------\n",
    "Version: V1.7       Modified: 06.11.2024        William Siegle\n",
    "---------------------------------------------------------------------\n",
    "notebook can be converted to python script using: \n",
    "(python -m) jupytext --to py FILENAME.ipynb\n",
    "------------------------------------------------------------------'''\n",
    "from pathlib import Path\n",
    "\n",
    "# SETTINGS ------------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    # SYSTEM: ---------------------------------------------------------------------\n",
    "    \"ROOT\":             Path('../..').resolve(),\n",
    "    \"INPUT_LOCATION\":   Path(\"TripSequences\", \"trips_processed_final\"), \n",
    "    \"OUTPUT_LOCATION\":  Path(\"src\", \"models\", \"pth\"),\n",
    "    \"GPU_SELECT\":       0, # {0,1,2,3, None: CPU only}\n",
    "    \"SEED\"  :           144,\n",
    "    \"PLOT_ACTIVE\":      True,\n",
    "\n",
    "    # DATA PREPROCESSING: ---------------------------------------------------------\n",
    "    \"TRAIN_VAL_TEST\":   [0.8, 0.15, 0.05], # [train, val, test splits]\n",
    "    \"MAX_FILES\":        2001, # None: all files\n",
    "    \"SCALERS\":          {'feature_scaler': 'MaxAbsScaler()', 'target_scaler': 'MinMaxScaler(feature_range=(0, 1))'},\n",
    "    \"MIN_SEQ_LENGTH\":   60, # minimum sequence length in seconds\n",
    "\n",
    "    # FEATURES: -------------------------------------------------------------------\n",
    "    \"FEATURES\":         ['actdrvtrnpwrprc_cval','hv_curr_cval_dcl1','hv_batpwr_cval_bms1','altitude_cval_ippc','motortemperature_pti1',\n",
    "                        'hv_batcurr_cval_bms1','hv_dclink_volt_cval_dcl1','brktempra_cval','hv_batmaxdischrgpwrlim_cval_1',\n",
    "                        'hv_batmaxchrgpwrlim_cval_1','hv_batisores_cval_e2e','emot_pwr_cval','actualtorque_pti1','powerstagetemperature_pti1',\n",
    "                        'accelpdlposn_cval','airtempoutsd_cval_cpc','airtempinsd_cval_hvac','actualdcvoltage_pti1','hv_batavcelltemp_cval_bms1',\n",
    "                        'epto_pwr_cval','maxtracpwrpct_cval','airtempinsd_rq','bs_brk_cval','brc_stat_brc1','vehspd_cval_cpc','hv_pwr_cval_dcl1',\n",
    "                        'lv_convpwr_cval_dcl1','vehweight_cval_pt','hv_bat_dc_momvolt_cval_bms1','actualspeed_pti1','selgr_rq_pt',\n",
    "                        'rmsmotorcurrent_pti1','roadgrad_cval_pt','currpwr_contendrnbrkresist_cval','elcomp_pwrcons_cval',\n",
    "                        'hv_ptc_cabin1_pwr_cval','maxrecuppwrprc_cval','txoiltemp_cval_tcm'],\n",
    "    \"TARGETS\":          ['hv_bat_soc_cval_bms1'],\n",
    "\n",
    "    # MODEL: -----------------------------------------------------------------------\n",
    "    \"HIDDEN_SIZE\":      400,    # features in the hidden state h\n",
    "    \"NUM_LAYERS\":       2,      # recurrent layers for stacked LSTMs. Default: 1\n",
    "    \"DROPOUT\":          0.5,\n",
    "    \"SEQ_LENGTH\":       60,\n",
    "    \n",
    "    # TRAINING & OPTIMIZER: --------------------------------------------------------\n",
    "    \"NUM_EPOCHS\":       2,\n",
    "    \"BATCH_SIZE\":       32,   # [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "    \"LEARNING_RATE\":    3e-3,   # 0.001 lr\n",
    "    \"OPTIMIZER\":        \"torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = 1e-3)\",      \n",
    "                        # weight_decay = 1e-4     # weight decay coefficient (default: 1e-2)\n",
    "                        # betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "                        # eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"LRSCHEDULER\":      \"torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 2, factor = 0.5, min_lr = 1e-7)\",\n",
    "\n",
    "    # LOSS FUNCTION: ---------------------------------------------------------------\n",
    "    \"CRITERION\":        \"nn.SmoothL1Loss()\", # ['nn.MSELoss()', 'nn.L1Loss()', 'nn.SmoothL1Loss()', 'nn.HuberLoss()', 'MASE()']\n",
    "    \"LOSS_FN\":          \"F.mse_loss(output, target)\", # ['F.mse_loss(output, target)', 'F.l1_loss(output, target)', 'F.smooth_l1_loss(output, target)', 'F.huber_loss(output, target)', 'F.mase_loss(output, target)']\n",
    "\n",
    "    # METRICS: ---------------------------------------------------------------------\n",
    "\n",
    "    # SAVE & LOAD: -----------------------------------------------------------------\n",
    "\n",
    "}\n",
    "for key in CONFIG: globals()[key] = CONFIG[key]; \n",
    "print(f\"ROOT: {ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "LOCATE DEVICES & SYSTEM FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Directories:\n",
      "  C:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN:\t\t\t.git, archive, data, project, ref, src, test\n",
      "  C:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-Data:\t\t\tOLD, processed, TripSequences\n",
      "------------------------------------------------------------\n",
      "Running in notebook mode\n"
     ]
    }
   ],
   "source": [
    "# LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  ---------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "import sys, os\n",
    "global ROOT, DATA_PATH\n",
    "if 'ROOT' not in globals(): ROOT = Path('../..').resolve()\n",
    "print(f\"{'-'*60}\\nDirectories:\\n  {ROOT}:\\t\\t\\t{', '.join([_.name for _ in ROOT.glob('*/')])}\")\n",
    "sys.path.append(os.path.abspath(ROOT))\n",
    "from data import get_data_path  # paths set in \"data/__init__.py\"\n",
    "DATA_PATH = get_data_path()\n",
    "print(f\"  {DATA_PATH}:\\t\\t\\t{', '.join([_.name for _ in DATA_PATH.glob('*/')])}\")\n",
    "\n",
    "# INTERNAL MODULE IMPORTS ----------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "from src.utils.data_utils import *\n",
    "from src.utils.train_model import *\n",
    "\n",
    "# NOTEBOOK / SCRIPT SETTINGS -------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "global IS_NOTEBOOK\n",
    "IS_NOTEBOOK = False\n",
    "try:    # if running in IPython\n",
    "    shell = get_ipython().__class__.__name__ # type: ignore \n",
    "    from IPython.display import display, HTML, Javascript, clear_output\n",
    "    from IPython.core.magic import register_cell_magic\n",
    "    @register_cell_magic    # cells can be skipped by using '%%skip' in the first line\n",
    "    def skip(line, cell): return\n",
    "    from tqdm.notebook import tqdm as tqdm_nb\n",
    "    IS_NOTEBOOK = True\n",
    "    print(f\"{'-'*60}\\nRunning in notebook mode\")\n",
    "except (NameError, ImportError):    # if running in script\n",
    "    from tqdm import tqdm as tqdm\n",
    "    from tabulate import tabulate\n",
    "    print(f\"{'-'*60}\\nRunning in script mode\")\n",
    "    \n",
    "# GENERAL MODULE IMPORTS -----------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "import math, time, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import pyarrow.parquet as pq\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "torch.set_default_dtype(torch.float32); torch.set_printoptions(precision=6, sci_mode=True)\n",
    "torch.manual_seed(SEED); random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "from pytorch_forecasting.metrics import MASE\n",
    "#from darts import TimeSeries\n",
    "#from darts.models import ExponentialSmoothing\n",
    "\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Torch version:  2.5.1+cpu\n",
      "Using: -->  CPU\n"
     ]
    }
   ],
   "source": [
    "# DEVICE SELECTION ---------------------------------------------------------------------\n",
    "global DEVICE\n",
    "print(f\"{'-'*60}\\nTorch version: \", torch.__version__)\n",
    "if not torch.cuda.is_available() or GPU_SELECT is None: DEVICE = torch.device(\"cpu\")\n",
    "else: DEVICE = torch.device(f\"cuda:{GPU_SELECT}\")\n",
    "print(f\"Using: -->  {str(DEVICE).upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input Data:\tC:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-Data\\TripSequences\\trips_processed_final\n",
      "Store model in:\tC:\\Users\\SIEGLEW\\OneDrive - Daimler Truck\\MA\\Code\\MA-eR-PINN\\src\\models\\pth\n",
      "------------------------------------------------------------\n",
      "Total Files:\t2001\n",
      "Filtered Files:\t2001\n",
      "------------------------------------------------------------\n",
      "               FileName  Length  Index\n",
      "0       V13_T25.parquet   20843   1669\n",
      "1     V18_T1224.parquet   16645   1074\n",
      "2      V13_T992.parquet   15726   1167\n",
      "3      V14_T887.parquet   15629   1338\n",
      "4      V13_T772.parquet   15439   1482\n",
      "...                 ...     ...    ...\n",
      "1996   V14_T825.parquet     120    966\n",
      "1997  V14_T1858.parquet     119   1706\n",
      "1998  V14_T1449.parquet     117    118\n",
      "1999   V13_T127.parquet     112    422\n",
      "2000   V16_T523.parquet     112    503\n",
      "\n",
      "[2001 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, INPUT_LOCATION) # Trip parquet files\n",
    "pth_folder = Path(ROOT, OUTPUT_LOCATION)\n",
    "print(f\"{'-'*60}\\nInput Data:\\t{input_folder}\\nStore model in:\\t{pth_folder}\")\n",
    "\n",
    "# PREPARE TRAIN & TEST SET ---------------------------------------------------\n",
    "all_files = [Path(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "if MAX_FILES is not None: files = random.sample(all_files, MAX_FILES)\n",
    "print(f\"{'-'*60}\\nTotal Files:\\t{len(files)}\")\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_parquet(Path(input_folder, random.choice(files)), engine='fastparquet')\n",
    "all_signals = df.columns; assert len(all_signals) == 44\n",
    "\n",
    "# FILTER INPUT FILES --------------------------------------------------------\n",
    "# generate lengths of all files by reading metadata or using presaved lengths\n",
    "try:\n",
    "    presaved_lengths = pd.read_pickle(Path(ROOT, 'data', 'df_files_lengths.pickle'))\n",
    "    presaved_lengths = presaved_lengths.set_index('FileName').to_dict()['Length']\n",
    "    trip_lengths = [presaved_lengths[file.name] for file in files]\n",
    "except:\n",
    "    print(f\"{'-'*60}\\nObtaining sequence lengths... (may take up to 5 minutes)\")\n",
    "    trip_lengths = [pq.read_metadata(file).num_rows for file in files]\n",
    "\n",
    "# discard all items shorter than min_seq_length\n",
    "filtered_files = []\n",
    "filtered_lengths = []\n",
    "for file, length in zip(files, trip_lengths):\n",
    "    if length > MIN_SEQ_LENGTH: \n",
    "        filtered_files.append(file)\n",
    "        filtered_lengths.append(length)\n",
    "\n",
    "# replace lists with only filtered items\n",
    "files = filtered_files\n",
    "trip_lengths = filtered_lengths\n",
    "print(f\"Filtered Files:\\t{len(files)}\\n{'-'*60}\")\n",
    "\n",
    "# SORT INPUT FILES BY SEQUENCE LENGTH --------------------------------------\n",
    "# this is needed in order to later sort the sequence by their length\n",
    "file_length_mapping = sorted([(file.name, length, idx) for idx, (file, length) in enumerate(zip(files, trip_lengths))], \\\n",
    "    key=lambda x: x[1], reverse=True)\n",
    "\n",
    "file_length_df = pd.DataFrame(file_length_mapping, columns=['FileName', 'Length', 'Index'])\n",
    "indices_by_length = file_length_df['Index'].to_list()\n",
    "sorted_trip_lengths = file_length_df['Length'].to_list()\n",
    "print(file_length_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT & TARGET SPECIFICATION ---------------------------------------------------\n",
    "# these signals are required for the physical Model calculation:\n",
    "base_signals = [\"signal_time\", \"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \n",
    "                \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "\n",
    "# these signals have to be dropped in order for appropriate training:\n",
    "columns_to_drop = [\"hv_batmomavldischrgen_cval_1\", \"latitude_cval_ippc\", \"longitude_cval_ippc\", \"signal_time\", \"hirestotalvehdist_cval_icuc\"]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "selection_1 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", 'roadgrad_cval_pt', \"vehweight_cval_pt\", \"accelpdlposn_cval\", \"bs_brk_cval\", \"elcomp_pwrcons_cval\",\n",
    "               \"epto_pwr_cval\", \"motortemperature_pti1\", \"powerstagetemperature_pti1\", 'airtempinsd_cval_hvac', 'brktempra_cval', 'selgr_rq_pt']\n",
    "selection_2 = [\"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\", \"roadgrad_cval_pt\"]\n",
    "selection_3 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"vehweight_cval_pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SELECTION  ----------------------------------------------------------------------------\n",
    "TARGET_COLUMN = TARGETS\n",
    "INPUT_COLUMNS = FEATURES\n",
    "#INPUT_COLUMNS = list(set(all_signals) - set(columns_to_drop) - set(TARGET_COLUMN))\n",
    "\n",
    "# FEATURE NORMALIZATION/SCALING -----------------------------------------------------------------\n",
    "scaler = eval(SCALERS['feature_scaler'])\n",
    "target_scaler = eval(SCALERS['target_scaler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting Scalers: MaxAbsScaler, MinMaxScaler\n",
      "\t50% of the fitting done...\n",
      "Done. Create DataSets and DataLoaders...\n",
      "\tNumber of batches created: 50\n",
      "\tNumber of batches created: 10\n",
      "\tNumber of batches created: 4\n"
     ]
    }
   ],
   "source": [
    "# GENERATE DATALOADERS ---------------------------------------------------------------\n",
    "\n",
    "# DATA SET SPLITTING AND SORTING ----------------------------------------------------------------\n",
    "train_subset, val_subset, test_subset = random_split(files, TRAIN_VAL_TEST)\n",
    "\n",
    "# DATALOADER SETTINGS ------------------------------------------------------------------\n",
    "dataloader_settings = {\n",
    "    'batch_size': 1,                # see *Note above\n",
    "    'shuffle': True,                # shuffle the batches before each epoch\n",
    "    'collate_fn': collate_fn,       # include optional arguments\n",
    "    'num_workers': 4,               # number of workers\n",
    "    'pin_memory': False if DEVICE.type == 'cpu' else True\n",
    "}\n",
    "\n",
    "# PREPARE TRAIN, VAL & TEST DATALOADERS  ------------------------------------------------------------\n",
    "train_subset, train_dataset, train_dataset_batches, train_loader = prepare_dataloader(train_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, scaler, target_scaler, dataloader_settings, fit=True)\n",
    "\n",
    "val_subset, val_dataset, val_dataset_batches, val_loader = prepare_dataloader(val_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, scaler, target_scaler, dataloader_settings, fit=False)\n",
    "\n",
    "test_subset, test_dataset, test_dataset_batches, test_loader = prepare_dataloader(test_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, scaler, target_scaler, dataloader_settings, fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train size:  3813717\t\t(Files: 1600)\n",
      "Val. size:   724977\t\t(Files: 300)\n",
      "Test size:   232163\t\t(Files: 100) \n",
      " ------------------------------------------------------------\n",
      "\tRemoved 1 file from the dataset\n",
      "------------------------------------------------------------\n",
      "first 3 train files: ['V13_T25.parquet', 'V18_T1224.parquet', 'V13_T992.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the datasets ----------------------------------\n",
    "print(f\"{'-'*60}\\nTrain size:  {len(train_dataset)}\\t\\t(Files: {len(train_subset)})\")\n",
    "print(f'Val. size:   {len(val_dataset)}\\t\\t(Files: {len(val_subset)})')\n",
    "print(f'Test size:   {len(test_dataset)}\\t\\t(Files: {len(test_subset)}) \\n {\"-\"*60}')\n",
    "\n",
    "if train_dataset.__len__() != sum(len(data) for data in train_dataset.data): print(\"Warning: Train Dataset Length Mismatch\")\n",
    "if len(train_subset)+len(val_subset)+len(test_subset) != len(files): \n",
    "    print(f\"\\tRemoved {len(files) - (len(train_subset)+len(val_subset)+len(test_subset))} file from the dataset\\n{'-'*60}\")\n",
    "    \n",
    "subset_files = {\"train_files\": list(train_dataset.file_list), \"val_files\": list(val_dataset.file_list), \"test_files\": list(test_dataset.file_list)}\n",
    "print(f\"first 3 train files: {[os.path.basename(_) for _ in subset_files['train'][:3]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "if IS_NOTEBOOK and PLOT_ACTIVE:\n",
    "\n",
    "    # SEQUENCE PADDING: VISUALIZATION ----------------------------------\n",
    "    # compare padding proportions for unsorted and sorted sequences, as well as for the train, val, and test sets\n",
    "    # Calculate and print the ratio between blue and orange areas for all diagrams\n",
    "    _ = plot_padded_sequences(BATCH_SIZE, trip_lengths, \"padding values (unsorted)\")\n",
    "    _ = plot_padded_sequences(BATCH_SIZE, sorted_trip_lengths, \"padding values (sorted)\")\n",
    "    _ = plot_padded_sequences(BATCH_SIZE, get_trip_lengths_from_loader(train_loader), \"padding values (Train Set)\")\n",
    "    _ = plot_padded_sequences(BATCH_SIZE, get_trip_lengths_from_loader(val_loader), \"padding values (Val Set)\")\n",
    "    _ = plot_padded_sequences(BATCH_SIZE, get_trip_lengths_from_loader(test_loader), \"padding values (Test Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "if IS_NOTEBOOK and None:\n",
    "    \n",
    "    # BATCH LOADER CHECK -----------------------------------------------------------------------\n",
    "    # Iterate through the train_loader once and print a batch example of a PackedSequence\n",
    "    for batch_idx, (packed_inputs, padded_targets, lengths) in enumerate(train_loader):\n",
    "        print(f\"Batch {batch_idx}\")\n",
    "        print(f\"Shape of packed_inputs.data: {packed_inputs.data.shape}\")\n",
    "        print(f\"Lengths: {lengths}\")\n",
    "        # check correct types and shapes\n",
    "        assert type(packed_inputs) == torch.nn.utils.rnn.PackedSequence\n",
    "        assert type(packed_inputs.data) == torch.Tensor\n",
    "        assert type(packed_inputs.batch_sizes) == torch.Tensor\n",
    "        assert type(padded_targets) == torch.Tensor\n",
    "        assert type(lengths) == torch.Tensor\n",
    "        assert len(packed_inputs.batch_sizes) == max(lengths)\n",
    "        assert sum(lengths) == packed_inputs.data.shape[0]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Move to model defintions\n",
    "'''\n",
    "# LSTM NETWORK -----------------------------------------------------------------------\n",
    "class LSTM1_packed(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, seq_length, dropout, device=DEVICE):\n",
    "        super(LSTM1_packed, self).__init__()\n",
    "\n",
    "        self.input_size = input_size    # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.num_layers = num_layers    # number of layers\n",
    "        self.dropout = dropout\n",
    "        self.seq_length = seq_length    # sequence length\n",
    "\n",
    "        # LSTM CELL --------------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size,            # The number of expected features in the input x\n",
    "            self.hidden_size,           # The number of features in the hidden state h\n",
    "            self.num_layers,            # Number of recurrent layers for stacked LSTMs. Default: 1\n",
    "            batch_first=True,           # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Default: False\n",
    "            bias=True,                  # If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            dropout=self.dropout,       # usually: [0.2 - 0.5], introduces a Dropout layer on the outputs of each LSTM layer except the last layer, (dropout probability). Default: 0\n",
    "            bidirectional=False,        # If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size=0,                # If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_test = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, packed_input, batch_size=None):\n",
    "        # Propagate input through LSTM\n",
    "        packed_out, _ = self.lstm(packed_input)\n",
    "        #print(f\"LSTM: Output after LSTM: {packed_out.data.shape}, {type(packed_out)}\")\n",
    "\n",
    "        # Unpack the output\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        #print(f\"             after packing: {out.shape}, {type(out)}\")\n",
    "\n",
    "        # Output layers\n",
    "        out = self.relu(out)  # relu\n",
    "        #print(f\"             after relu: {out.shape}, {type(out)}\")\n",
    "\n",
    "        out = self.fc_test(out)  # Use all outputs for prediction\n",
    "        #print(f\"             after fc: {out.shape}, {type(out)}\")\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " LSTM1_packed(\n",
      "  (lstm): LSTM(38, 400, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (relu): ReLU()\n",
      "  (fc_test): Linear(in_features=400, out_features=1, bias=True)\n",
      ") ------------------------------------------------------------\n",
      "Model state_dict:\n",
      "lstm.weight_ih_l0:\t torch.Size([1600, 38])\n",
      "lstm.weight_hh_l0:\t torch.Size([1600, 400])\n",
      "lstm.bias_ih_l0:\t torch.Size([1600])\n",
      "lstm.bias_hh_l0:\t torch.Size([1600])\n",
      "lstm.weight_ih_l1:\t torch.Size([1600, 400])\n",
      "lstm.weight_hh_l1:\t torch.Size([1600, 400])\n",
      "lstm.bias_ih_l1:\t torch.Size([1600])\n",
      "lstm.bias_hh_l1:\t torch.Size([1600])\n",
      "fc_test.weight:\t torch.Size([1, 400])\n",
      "fc_test.bias:\t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# MODEL CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# LAYERS --------------------------------\n",
    "input_size = len(INPUT_COLUMNS)     # expected features in the input x\n",
    "hidden_size = HIDDEN_SIZE           # features in the hidden state h\n",
    "num_layers = NUM_LAYERS             # recurrent layers for stacked LSTMs. Default: 1\n",
    "num_classes = 1                     # output classes (=1 for regression)\n",
    "\n",
    "# INSTANTIATE MODEL --------------------\n",
    "model = LSTM1_packed(input_size, HIDDEN_SIZE, NUM_LAYERS, SEQ_LENGTH, DROPOUT).to(DEVICE)\n",
    "print(f\"{'-'*60}\\n\", model, f\"{'-'*60}\\nModel state_dict:\")\n",
    "for param_tensor in model.state_dict(): print(f\"{param_tensor}:\\t {model.state_dict()[param_tensor].size()}\") \n",
    "# --> Note torch.Size([4*hidden_size, input_size]) for LSTM weights because of i,o,f,g params concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.003\n",
      "    maximize: False\n",
      "    weight_decay: 0.001\n",
      ")\n",
      "------------------------------------------------------------\n",
      "LRScheduler: <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "------------------------------------------------------------\n",
      "Criterion: <class 'torch.nn.modules.loss.SmoothL1Loss'>\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# OPTIMIZER --------------------------------------------------------------------------------\n",
    "# common optimizers: ['torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.RMSprop']\n",
    "if 'OPTIMIZER' in globals(): optimizer = eval(OPTIMIZER)\n",
    "else: optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE,\n",
    "        weight_decay = 1e-4      # weight decay coefficient (default: 1e-2)\n",
    "        #betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        #eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    ")\n",
    "print(f\"{'-'*60}\\n{optimizer}\\n{'-'*60}\")\n",
    "\n",
    "# LR SCHEDULER ----------------------------------------------------------------------------\n",
    "if 'LRSCHEDULER' in globals(): scheduler = eval(LRSCHEDULER)\n",
    "else: scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 2, factor = 0.5, min_lr = 1e-6)\n",
    "print(f\"LRScheduler: {scheduler.__class__}\\n{'-'*60}\")\n",
    "\n",
    "# LOSS FUNCTION ----------------------------------------------------------------\n",
    "def loss_fn(output, target):\n",
    "    if 'LOSS_FN' in globals(): loss = eval(LOSS_FN)\n",
    "    else: loss = F.mse_loss(output, target) # mean-squared error for regression\n",
    "    return loss\n",
    "\n",
    "# or define criterion function:\n",
    "criterion_list = [nn.MSELoss(), nn.L1Loss(), nn.SmoothL1Loss(), nn.HuberLoss(), MASE()]\n",
    "\n",
    "if 'CRITERION' in globals(): criterion = eval(CRITERION)\n",
    "else: criterion = nn.SmoothL1Loss()\n",
    "print(f\"Criterion: {criterion.__class__}\\n{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET NETWORK TRAINER -----------------------------------------------------------------\n",
    "TRAINER = Trainer_packed(\n",
    "    model = model, \n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler, \n",
    "    loss_fn = criterion, \n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    num_epochs = NUM_EPOCHS, \n",
    "    device = DEVICE, \n",
    "    is_notebook = IS_NOTEBOOK,\n",
    "    use_mixed_precision = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING -----------------------------------------------------------------\n",
    "CHECKPOINT = TRAINER.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CHECKPOINT -----------------------------------------------------------------\n",
    "\n",
    "# COLLECTING RESULTS AND META DATA ---------------------------------------------------\n",
    "trainer_add_info = {key: getattr(TRAINER, key) for key in \\\n",
    "    ['clip_value', 'device', 'optimizer', 'scheduler', 'state', 'test_loader', 'train_loader', 'use_mixed_precision', 'val_loader']}\n",
    "CHECKPOINT['CONFIG'] = CONFIG\n",
    "CHECKPOINT = {**CHECKPOINT, **trainer_add_info, **subset_files}\n",
    "\n",
    "# SAVE   -----------------------------------------------------------------\n",
    "# create unique identifier for model name\n",
    "model_name_id = f'{model.__class__.__name__}_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}'\n",
    "model_destination_path = Path(pth_folder, model_name_id + \".pth\")\n",
    "\n",
    "# save object & print info\n",
    "torch.save(CHECKPOINT, model_destination_path)\n",
    "print(f\"Model saved to:\\t {model_destination_path}\\n{'-'*60}\\nSize: {os.path.getsize(model_destination_path) / 1024**2:.2f} MB\\n{'-'*60}\")\n",
    "if os.path.getsize(model_destination_path) > 100 * 1024**2: print(\"--> Warning: The saved model size exceeds 100MB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CHECKPOINT -----------------------------------------------------------------\n",
    "model_destination_path = Path(pth_folder, \"LSTM1_packed_241129_211619.pth\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "checkpoint = torch.load(model_destination_path, weights_only=False, map_location=DEVICE if torch.cuda.is_available() else torch.device('cpu'))\n",
    "for key in [\"model\", \"loss_fn\", \"training_table\", \"train_losses\", \"train_losses_per_iter\", \"val_losses\", \"epoch\", \"lr_history\"]: \n",
    "    exec(f\"{key} = checkpoint[key]\")\n",
    "\n",
    "# configure model and optimizer:\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Move the model and its parameters to the CPU      -->  necessary?\n",
    "try:\n",
    "    if torch.cuda.is_available() and GPU_SELECT is not None:\n",
    "        model.to(DEVICE)\n",
    "    else:\n",
    "        model.to(torch.device(\"cpu\"))\n",
    "except:\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "\n",
    "model.eval(); # set model to evaluation mode for inference\n",
    "print(f\"Model loaded from:\\t {model_destination_path}\\n{'-'*60}\")\n",
    "print(f\"Model: {model.__class__.__name__}\\t\\tParameters on device: {next(model.parameters()).device}\\n{'-'*60}\\n\"\n",
    "        f\"Train/Batch size:\\t{len(train_loader.dataset)} / {train_loader.batch_size}\\n\"\n",
    "        f\"Loss:\\t\\t\\t{loss_fn}\\nOptimizer:\\t\\t{optimizer.__class__.__name__}\\nLR:\\t\\t\\t\"\n",
    "        f\"{optimizer.param_groups[0]['lr']}\\nWeight Decay:\\t\\t{optimizer.param_groups[0]['weight_decay']}\\n{'-'*60}\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DataFrame of training metrics:\n",
    "training_df = pd.DataFrame(training_table, columns=[\"Epoch\", \"Iteration\", \"Batch Loss\", \"Train Loss\"])\n",
    "# Extract the 'Train Loss' column and compare with the train_losses list\n",
    "train_loss_column = training_df['Train Loss'].replace(['',' '], np.nan).dropna().astype(float).values\n",
    "if any(abs(train_loss_column - train_losses) > 1e-3):  print(\"Extracted and original Train Losses are not equal. Please check metrics table.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# plot training performance:\n",
    "fig, ax1 = plt.subplots(figsize=(14,4))\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_xticks(range(1, NUM_EPOCHS + 1))\n",
    "\n",
    "assert len(train_losses_per_iter) == NUM_EPOCHS * len(train_loader), \"Length of train_losses_per_iter might not match the number of iterations.\"\n",
    "plt.plot(np.linspace(1, NUM_EPOCHS, len(train_losses_per_iter)), train_losses_per_iter, label='batch_loss', color='lightblue')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='train_loss', color='blue')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), val_losses, label='val_loss', color='red')\n",
    "\n",
    "plt.yscale('log'); fig.tight_layout(); plt.legend();\n",
    "\n",
    "plt.text(0.86, 0.6, f\"Train: {train_losses[-1]:.3e}\\nVal:    {val_losses[-1]:.3e}\", \\\n",
    "    transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5));\n",
    "\n",
    "if pd.Series(lr_history).nunique() > 1:\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(1, NUM_EPOCHS + 1), lr_history, label='lr', color='green', linestyle='--')\n",
    "    ax2.set_ylabel('Learning Rate', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    ax2.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION -----------------------------------------------------------------\n",
    "# get file list of test subset\n",
    "test_files = checkpoint[\"subset_files\"][\"test\"]\n",
    "# -------------------------------------\n",
    "test_loss, outputs, targets, original_lengths = trainer.evaluate_model()\n",
    "# -------------------------------------\n",
    "all_outputs, all_targets, all_original_lengths = [], [], []\n",
    "for batch_outputs, batch_targets, batch_lengths in zip(outputs, targets, original_lengths):\n",
    "    all_outputs.extend(batch_outputs)\n",
    "    all_targets.extend(batch_targets)\n",
    "    all_original_lengths.extend(batch_lengths)\n",
    "\n",
    "# Inverse-transform on all outputs and targets for evaluation\n",
    "#scaled_outputs = target_scaler.inverse_transform(np.concatenate(all_outputs, axis=0).reshape(-1, 1))\n",
    "#scaled_targets = target_scaler.inverse_transform(np.concatenate(all_targets, axis=0).reshape(-1, 1))\n",
    "\n",
    "# Inverse-transform on all outputs and targets for evaluation\n",
    "scaled_outputs = [target_scaler.inverse_transform(output_sequence.reshape(1, -1)).squeeze() for output_sequence in all_outputs]\n",
    "scaled_targets = [target_scaler.inverse_transform(target_sequence.reshape(1, -1)).squeeze() for target_sequence in all_targets]\n",
    "\n",
    "\n",
    "print(f\"Test Loss:  {test_loss:.4f}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(np.concatenate(scaled_targets), np.concatenate(scaled_outputs)):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(np.concatenate(scaled_targets) - np.concatenate(scaled_outputs)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sample sequence from test set\n",
    "# -------------------------------------\n",
    "sample_int = random.randint(1, len(scaled_outputs))\n",
    "y_pred = scaled_outputs[sample_int]\n",
    "y_true = scaled_targets[sample_int]\n",
    "\n",
    "###############################################\n",
    "# PLOT PREDICTION -----------------------------------------------------------------\n",
    "if PLOT_ACTIVE:\n",
    "     plt.figure(figsize=(18,4)); plt.xlabel('Time in s'); plt.ylabel('SOC in %'); plt.title('Battery State of Charge: Prediction vs. Actual Data') \n",
    "     plt.plot(y_true, label='Actual Data') # actual plot\n",
    "     plt.plot(np.arange(0, len(y_true), 1), y_pred, label='Predicted Data') # predicted plot\n",
    "     plt.legend()\n",
    "     plt.text(0.01, 0.02, f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\\nStd Dev: {np.std(y_true - y_pred):.4f}\",\\\n",
    "          transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "     plt.figure(figsize=(18,4)); plt.xlabel('Time in s'); plt.ylabel('SOC in %')\n",
    "     plt.plot(savgol_filter(y_true.flatten(), window_length=60, polyorder=3), label='Actual Data (Smoothed)') # actual plot\n",
    "     plt.plot(np.arange(0, len(y_true), 1), savgol_filter(y_pred.flatten(), window_length=60, polyorder=3), label='Predicted Data (Smoothed)') # predicted plot\n",
    "     plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODOs\n",
    "    - check if any batch contains only one sequence. If so, discard it before training DONE\n",
    "    - val loss must be significantly higher than train loss !!!\n",
    "    - check effects of mixed precision training >> tbd\n",
    "''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
