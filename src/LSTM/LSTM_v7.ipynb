{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting.\n"
     ]
    }
   ],
   "source": [
    "'''------------------------------------------------------------------\n",
    "LSTM Training\n",
    "Version: V1.7\n",
    "Modified: 06.11.2024\n",
    "William Siegle\n",
    "---------------------------------------------------------------------\n",
    "notebook can be converted to python script using: \n",
    "(python -m) jupytext --to py FILENAME.ipynb\n",
    "------------------------------------------------------------------'''\n",
    "print(\"Starting.\");\n",
    "# SETTINGS ---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS ---------------------------------------------------------------------\n",
    "global IS_NOTEBOOK\n",
    "IS_NOTEBOOK = False\n",
    "try:    # if running in IPython\n",
    "    shell = get_ipython().__class__.__name__ # type: ignore \n",
    "    #%reset -f -s\n",
    "    %matplotlib inline\n",
    "    from IPython.display import display, HTML, Javascript\n",
    "    from IPython.core.magic import register_cell_magic\n",
    "    from IPython.display import clear_output\n",
    "    @register_cell_magic    # cells can be skipped by using '%%skip' in the first line\n",
    "    def skip(line, cell): return\n",
    "    from tqdm.notebook import tqdm # type: ignore\n",
    "    IS_NOTEBOOK = True\n",
    "except (NameError, ImportError):    # if running in script\n",
    "    from tqdm import tqdm\n",
    "    from tabulate import tabulate\n",
    "    print(f\"{'-'*60}\\nRunning in script mode\")\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "torch.set_default_dtype(torch.float32)\n",
    "#torch.manual_seed(2);\n",
    "torch.manual_seed(datetime.now().second);\n",
    "\n",
    "\n",
    "# METRICS ---------------------------------------------------------------------\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "#from pytorch_forecasting.metrics import MASE\n",
    "\n",
    "# DARTS ---------------------------------------------------------------------\n",
    "from darts.metrics import mase\n",
    "from darts import TimeSeries\n",
    "from darts.models import ExponentialSmoothing\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.dataprocessing import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "LOCATE DEVICES & SYSTEM FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Torch version:  2.4.1+cu121\n",
      "Cuda available:  True\n",
      "Currently Selected Device: 0\n",
      "Using: -->  cuda\n"
     ]
    }
   ],
   "source": [
    "# DEVICE SELECTION ---------------------------------------------------------------------\n",
    "global DEVICE\n",
    "print(f\"{'-'*60}\\nTorch version: \", torch.__version__)\n",
    "print('Cuda available: ',torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(f\"cuda\") \n",
    "    #DEVICE = torch.device(\"cuda:1\")   # or overwrite with explicit Core number\n",
    "    print(f'Currently Selected Device: {torch.cuda.current_device()}')\n",
    "else:\n",
    "    DEVICE = (\"cpu\")\n",
    "print(f\"Using: -->  {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "/home/sieglew/MA-eR-PINN:\tproject, ref, test, data, src, .git, archive\n",
      "/home/sieglew/MA-Data:\t\t__pycache__, trips_processed_resampled, final, y_true, processed, trips_processed_pickles, final_2, trips_processed_final\n"
     ]
    }
   ],
   "source": [
    "# ------------ LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  --------------\n",
    "global ROOT, DATA_PATH\n",
    "ROOT = Path('../..').resolve() if IS_NOTEBOOK else Path('.').resolve()\n",
    "print(f\"{'-'*60}\\n{ROOT}:\\t{', '.join([_.name for _ in ROOT.glob('*/')])}\")\n",
    "sys.path.append(os.path.abspath(ROOT))\n",
    "from data import get_data_path  # paths set in \"data/__init__.py\"\n",
    "DATA_PATH = get_data_path()\n",
    "print(f\"{DATA_PATH}:\\t\\t{', '.join([_.name for _ in DATA_PATH.glob('*/')])}\")\n",
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input Data: /home/sieglew/MA-Data/final_2/trips_processed_resampled\n",
      "Store model in: /home/sieglew/MA-eR-PINN/src/models/pth\n"
     ]
    }
   ],
   "source": [
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, \"final_2\", \"trips_processed_resampled\") # Trip parquet files\n",
    "pth_folder = Path(ROOT, \"src\", \"models\", \"pth\")\n",
    "print(f\"{'-'*60}\\nInput Data: {input_folder}\\nStore model in: {pth_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Total Files: 2975\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PREPARE TRAIN & TEST SET ---------------------------------------------------\n",
    "all_files = [Path(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "files = all_files\n",
    "print(f\"{'-'*60}\\nTotal Files: {len(files)}\\n{'-'*60}\")\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_parquet(Path(input_folder, random.choice(files)), engine='fastparquet')\n",
    "all_signals = df.columns\n",
    "assert len(all_signals) == 57\n",
    "\n",
    "# get df stats:\n",
    "# df.info()\n",
    "'''from scipy.stats import shapiro\n",
    "nd = []\n",
    "for sig in df.columns:\n",
    "    if np.ptp(df[sig]) != 0:\n",
    "        _ , p = shapiro(df[sig])\n",
    "        if p > 0.05:\n",
    "            nd.append(sig)\n",
    "print(f\"{'-'*60}\\nNormal Distributed Signals: {len(nd)}\\n{'-'*60}\")''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT & TARGET SPECIFICATION ---------------------------------------------------\n",
    "# these signals are required for the physical Model calculation:\n",
    "base_signals = [\"signal_time\", \"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \n",
    "\"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "\n",
    "# these signals have to be dropped in order for appropriate training:\n",
    "columns_to_drop = [\"hv_bat_soc_cval_bms1\", \"latitude_cval_ippc\", \"longitude_cval_ippc\", \"signal_time\", \"signal_ts\"]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "target_column = \"hv_batmomavldischrgen_cval_1\"\n",
    "input_columns = all_signals.drop(columns_to_drop + [target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DEFINITION -----------------------------------------------------------------------\n",
    "class TripDataset(Dataset):\n",
    "    def __init__(self, file_list, scaler, target_scaler, fit=False):\n",
    "        self.file_list = file_list\n",
    "        self.scaler = scaler\n",
    "        self.target_scaler = target_scaler\n",
    "        self.fit = fit\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        if self.fit:\n",
    "            print(f\"fitting Scalers: {scaler.__class__.__name__}, {target_scaler.__class__.__name__}\")\n",
    "            # Initialize and Fit the scalers on the complete training data set\n",
    "            # Fit the scalers incrementally to avoid memory errors\n",
    "            for file in self.file_list:\n",
    "                df = pd.read_parquet(file, engine='fastparquet')\n",
    "                X = df[input_columns].values\n",
    "                y = df[target_column].values.reshape(-1, 1)  # Reshape to match the shape of the input\n",
    "                self.scaler.partial_fit(X)\n",
    "                self.target_scaler.partial_fit(y)\n",
    "            print(f\"Done. Create DataSets...\")\n",
    "\n",
    "        for file in self.file_list:\n",
    "            # DATA PREPROCESSING -----------------------------------------------------------\n",
    "            # Assigning inputs and targets and reshaping ---------------\n",
    "            df = pd.read_parquet(file, engine='fastparquet')\n",
    "            X = df[input_columns].values\n",
    "            y = df[target_column].values.reshape(-1, 1)  # Reshape to match the shape of the input\n",
    "            # use the previously fitted scalers to transform the data\n",
    "            X = self.scaler.transform(X)  \n",
    "            y = self.target_scaler.transform(y).squeeze()\n",
    "            # Append to data\n",
    "            self.data.append(X)\n",
    "            self.targets.append(y.squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(target) for target in self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find which file the index belongs to\n",
    "        # enables indexing over concatenated dataset via one timestep index\n",
    "        for i, target in enumerate(self.targets):\n",
    "            if index < len(target):\n",
    "                return (\n",
    "                    torch.tensor(self.data[i][index], dtype=torch.float32).unsqueeze(0),  # Add time dimension\n",
    "                    torch.tensor(target[index], dtype=torch.float32)\n",
    "                )\n",
    "            index -= len(target)\n",
    "        raise IndexError(\"Index out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE NORMALIZATION/SCALING -----------------------------------------------------------------\n",
    "scaler = MaxAbsScaler() \n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SET SPLITTING -----------------------------------------------------------------------\n",
    "# train_subset, test_subset = train_test_split(files, test_size=0.2, random_state=1)\n",
    "train_subset, val_subset, test_subset = random_split(files, [0.8, 0.15, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting Scalers: MaxAbsScaler, MinMaxScaler\n",
      "Done. Create DataSets...\n",
      "------------------------------------------------------------\n",
      "Train size:  10106504\t\t(Files: 2381)\n",
      "Val. size:   1802283\t\t(Files: 446)\n",
      "Test size:   551895\t\t(Files: 148) \n",
      " ------------------------------------------------------------\n",
      "['v_id983V12_trip54.parquet', 'v_id983V12_trip103.parquet', 'v_id983V101_trip89.parquet']\n"
     ]
    }
   ],
   "source": [
    "# GENERATE DATALOADERS  ---------------------------------------------------------------\n",
    "batch_size = 2048 # [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "# Note:\n",
    "#   the scaler will be fitted only on the training data set\n",
    "#   shuffling is prohibited to maintain the time series order\n",
    "\n",
    "# TRAIN  ------------------------------------------------------------\n",
    "train_dataset = TripDataset(train_subset, scaler, target_scaler, fit=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# VAL ------------------------------------------------------------\n",
    "val_dataset = TripDataset(val_subset, scaler, target_scaler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# TEST ------------------------------------------------------------\n",
    "test_dataset = TripDataset(test_subset, scaler, target_scaler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the size of the datasets ----------------------------------\n",
    "print(f\"{'-'*60}\\nTrain size:  {len(train_dataset)}\\t\\t(Files: {len(train_subset)})\")\n",
    "print(f'Val. size:   {len(val_dataset)}\\t\\t(Files: {len(val_subset)})')\n",
    "print(f'Test size:   {len(test_dataset)}\\t\\t(Files: {len(test_subset)}) \\n {\"-\"*60}')\n",
    "if train_dataset.__len__() != sum(len(data) for data in train_dataset.data): print(\"Warning: Train Dataset Length Mismatch\")\n",
    "\n",
    "subset_files = {\"train\":    list(train_loader.dataset.file_list),\n",
    "                \"val\":      list(val_loader.dataset.file_list),\n",
    "                \"test\":     list(test_loader.dataset.file_list)}\n",
    "print([os.path.basename(_) for _ in subset_files[\"train\"][:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM NETWORK -----------------------------------------------------------------------\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, device = DEVICE): #, num_classes, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "\n",
    "        self.input_size = input_size    # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.num_layers = num_layers    # number of layers\n",
    "        #self.num_classes = num_classes  # number of classes\n",
    "        #self.seq_length = seq_length    # sequence length\n",
    "\n",
    "        # LSTM CELL --------------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size,            # The number of expected features in the input x\n",
    "            self.hidden_size,           # The number of features in the hidden state h\n",
    "            self.num_layers,            # Number of recurrent layers for stacked LSTMs. Default: 1\n",
    "            batch_first = True,         # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Default: False\n",
    "            bias = True,                # If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            dropout = 0.30,             # usually: [0.2 - 0.5] ,introduces a Dropout layer on the outputs of each LSTM layer except the last layer, (dropout probability). Default: 0\n",
    "            bidirectional = False,      # If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size = 0,              # If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "            device = DEVICE) \n",
    "        \n",
    "        # --------------------------------\n",
    "        #self.fc_1 =  nn.Linear(hidden_size, 128)  # fully connected 1\n",
    "        #self.fc = nn.Linear(128, num_classes)     # fully connected last layer\n",
    "        # --------------------------------\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_test =  nn.Linear(hidden_size, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, input, batch_size = None):\n",
    "        '''        \n",
    "        # initial hidden and internal states\n",
    "        # --------------------------------\n",
    "        h_0 = torch.zeros(self.num_layers, input.size(0) if batch_size is None else batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, input.size(0) if batch_size is None else batch_size, self.hidden_size)  \n",
    "        # --------------------------------\n",
    "        out = self.relu(hn.view(-1, self.hidden_size)) # reshaping the data for Dense layer next\n",
    "        out = self.fc_1(out) # first Dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc(out) # Final Output\n",
    "        '''\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        # --------------------------------\n",
    "        # output, (hn, cn) = self.lstm(input, (h_0, c_0)) # lstm with input, hidden, and internal state\n",
    "        # input shape:      (batch_size, seq_length, input_size)\n",
    "        # output shape:     (batch_size, seq_length, hidden_size)\n",
    "        # --------------------------------\n",
    "        out, _ = self.lstm(input)\n",
    "\n",
    "\n",
    "        # ouput layers\n",
    "        # --------------------------------\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_test(out[:, -1, :])  \n",
    "        #out = self.fc_test(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN NETWORK -----------------------------------------------------------------------\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        \n",
    "        # here, our linear layer will have an input of 200, not 100 as before:\n",
    "        self.fc1 = nn.Linear(200,5)  \n",
    "        self.fc2 = nn.Linear(5,10)   \n",
    "        self.fc3 = nn.Linear(10,100) # but the output remains 100\n",
    "        \n",
    "          \n",
    "    def forward(self, x):\n",
    "        # we have to flatten our 20x2x100 to a 20x200:\n",
    "        x = x.view(x.size(0),-1)     # x.size(0) is 20, and -1 is a shortcut for \"figure out the other number for me please!\"\n",
    "        \n",
    "        # the rest proceeds as before:\n",
    "        x = F.relu(self.fc1(x))      \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)              \n",
    "        return x\n",
    "        \n",
    "#net = FCNN()\n",
    "#print(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# LAYERS --------------------------------\n",
    "input_size = len(input_columns)     # expected features in the input x\n",
    "hidden_size = 100                   # features in the hidden state h\n",
    "num_layers = 2                      # recurrent layers for stacked LSTMs. Default: 1\n",
    "num_classes = 1                     # output classes (=1 for regression)\n",
    "\n",
    "# INSTANTIATE MODEL --------------------\n",
    "model = LSTM1(input_size, hidden_size, num_layers).to(DEVICE)  #, num_classes, seq_length).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " LSTM1(\n",
      "  (lstm): LSTM(51, 100, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (fc_test): Linear(in_features=100, out_features=1, bias=True)\n",
      ") ------------------------------------------------------------\n",
      "Model state_dict:\n",
      "lstm.weight_ih_l0:\t torch.Size([400, 51])\n",
      "lstm.weight_hh_l0:\t torch.Size([400, 100])\n",
      "lstm.bias_ih_l0:\t torch.Size([400])\n",
      "lstm.bias_hh_l0:\t torch.Size([400])\n",
      "lstm.weight_ih_l1:\t torch.Size([400, 100])\n",
      "lstm.weight_hh_l1:\t torch.Size([400, 100])\n",
      "lstm.bias_ih_l1:\t torch.Size([400])\n",
      "lstm.bias_hh_l1:\t torch.Size([400])\n",
      "fc_test.weight:\t torch.Size([1, 100])\n",
      "fc_test.bias:\t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.005\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING CONFIGURATION -----------------------------------------------------------------------\n",
    "global NUM_EPOCHS\n",
    "\n",
    "# HYPERPARAMETERS ------------------------------------------------------------\n",
    "NUM_EPOCHS = 25\n",
    "learning_rate = 5e-3 # 0.001 lr\n",
    "\n",
    "# OPTIMIZER -----------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate,\n",
    "    weight_decay = 1e-4      # weight decay coefficient (default: 1e-2)\n",
    "    #betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    #eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    ")\n",
    "\n",
    "# LR SCHEDULER -----------------------------\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 2, factor = 0.5, min_lr = 1e-6)\n",
    "\n",
    "# LOSS FUNCTION ----------------------------------------------------------------\n",
    "def loss_fn(model_output, target):\n",
    "    loss = F.mse_loss(model_output, target) # mean-squared error for regression\n",
    "    return loss\n",
    "\n",
    "# or define criterion function:\n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.L1Loss()\n",
    "criterion3 = nn.SmoothL1Loss()\n",
    "criterion4 = nn.HuberLoss()\n",
    "\n",
    "criterion = criterion4\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# print Model and Optimizer state_dicts\n",
    "print(f\"{'-'*60}\\n\", model, f\"{'-'*60}\\nModel state_dict:\")\n",
    "for param_tensor in model.state_dict(): print(f\"{param_tensor}:\\t {model.state_dict()[param_tensor].size()}\")\n",
    "print(f\"{'-'*60}\\n{optimizer}\\n{'-'*60}\\n{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a HTML table for performance tracking (if running in a notebook)\n",
    "def initialize_table():\n",
    "    table_html = \"\"\"\n",
    "    <table id=\"training_table\" style=\"width:60%; border-collapse: collapse;\">\n",
    "        <thead style=\"position: sticky; top: 0; z-index: 1;\">\n",
    "            <tr>\n",
    "                <th style=\"font-weight:bold; width:15%; text-align:left; padding: 10px; background-color: #404040;\">Epoch</th>\n",
    "                <th style=\"font-weight:bold; width:25%; text-align:left; padding: 10px; background-color: #404040;\">Iteration</th>\n",
    "                <th style=\"font-weight:bold; width:30%; text-align:left; padding: 10px; background-color: #404040;\">Batch Loss</th>\n",
    "                <th style=\"font-weight:bold; width:30%; text-align:left; padding: 10px; background-color: #404040;\">Train Loss</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    <script>\n",
    "        function addRow(epoch, step, loss, running_loss) {\n",
    "            var table = document.getElementById(\"training_table\").getElementsByTagName('tbody')[0];\n",
    "            var row = table.insertRow(-1);\n",
    "            var cell1 = row.insertCell(0);\n",
    "            var cell2 = row.insertCell(1);\n",
    "            var cell3 = row.insertCell(2);\n",
    "            var cell4 = row.insertCell(3);\n",
    "            cell1.style.textAlign = \"left\";\n",
    "            cell2.style.textAlign = \"left\";\n",
    "            cell3.style.textAlign = \"left\";\n",
    "            cell4.style.textAlign = \"left\";\n",
    "            cell1.innerHTML = epoch;\n",
    "            cell2.innerHTML = step;\n",
    "            cell3.innerHTML = loss;\n",
    "            cell4.innerHTML = running_loss;\n",
    "            var scrollableDiv = document.getElementById(\"scrollable_table\");\n",
    "            scrollableDiv.scrollTop = scrollableDiv.scrollHeight;\n",
    "        }\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return \"\"\"<div id=\"scrollable_table\" style=\"height: 300px; overflow-y: scroll;\">\"\"\" + table_html + \"\"\"</div>\"\"\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "def add_row(table, epoch, iteration, batch_loss, train_loss):\n",
    "    table.append([epoch, iteration, batch_loss, train_loss])\n",
    "\n",
    "# Function to print the performance table\n",
    "header_printed = False\n",
    "def print_row(training_table):\n",
    "    global header_printed\n",
    "    headers = [\"Epoch\", \"Iteration\", \"Batch Loss\", \"Train Loss\"]\n",
    "    col_widths = [14, 14, 14, 14]  # Define fixed column widths\n",
    "\n",
    "    def format_row(row):\n",
    "        return [str(item).ljust(width) for item, width in zip(row, col_widths)]\n",
    "\n",
    "    if not header_printed:\n",
    "        formatted_headers = format_row(headers)\n",
    "        tqdm.write(tabulate([training_table[-1]], headers=formatted_headers, tablefmt=\"plain\", colalign=(\"left\", \"left\", \"left\", \"left\")))\n",
    "        header_printed = True\n",
    "    else:\n",
    "        formatted_row = format_row(training_table[-1])\n",
    "        tqdm.write(tabulate([training_table[-1]], headers=format_row([\"\", \"\", \"\", \"\"]), tablefmt=\"plain\", colalign=(\"left\", \"left\", \"left\", \"left\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ROUTINE DEFINITION -----------------------------------------------------------------\n",
    "def train_model(model, optimizer, scheduler, loss_fn, train_loader, val_loader = None, state=None):\n",
    "    def validate_model(model, val_loader, loss_fn):\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)  # Calculate average validation loss\n",
    "        return val_loss\n",
    "\n",
    "    # output info on training process\n",
    "    print(f\"Training Started.\\tProcess ID: {os.getpid()} \\n{'-'*60}\\n\"\n",
    "        f\"Model: {model.__class__.__name__}\\t\\tParameters on device: {next(model.parameters()).device}\\n{'-'*60}\\n\"\n",
    "        f\"Train/Batch size:\\t{len(train_loader.dataset)} / {train_loader.batch_size}\\n\"\n",
    "        f\"Loss:\\t\\t\\t{loss_fn}\\nOptimizer:\\t\\t{optimizer.__class__.__name__}\\nLR:\\t\\t\\t\"\n",
    "        f\"{optimizer.param_groups[0]['lr']}\\nWeight Decay:\\t\\t{optimizer.param_groups[0]['weight_decay']}\\n{'-'*60}\")\n",
    "\n",
    "    # Load state dict if provided\n",
    "    start_epoch = 1\n",
    "    if state:\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        start_epoch = state['epoch'] + 1\n",
    "        train_losses = state['train_losses']\n",
    "        val_losses = state['val_losses']\n",
    "        training_table = state['training_table']\n",
    "    else:\n",
    "        train_losses, val_losses, training_table = [], [], []  # collect loss\n",
    "        if IS_NOTEBOOK: display(HTML(initialize_table()))\n",
    "\n",
    "    # TRAINING LOOP:\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "        model.train()   # set model to training mode\n",
    "        running_loss = 0.0\n",
    "        num_iterations = math.ceil(len(train_loader.dataset) / train_loader.batch_size)\n",
    "        header_printed = False\n",
    "        \n",
    "        with tqdm(enumerate(train_loader, 1), unit=\"batch\", total=num_iterations, leave=False) as tepoch:\n",
    "            for iter, (inputs, targets) in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "                # -------------------------------------------------------------\n",
    "                # Move data to the GPU\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  \n",
    "                # zero gradients -> forward pass -> obtain loss function -> apply backpropagation -> update weights:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs) \n",
    "                loss = loss_fn(outputs.squeeze(), targets) \n",
    "                loss.backward() \n",
    "                # nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0) # optional: Gradient Value Clipping\n",
    "                optimizer.step()\n",
    "\n",
    "                # -------------------------------------------------------------\n",
    "                # Update the performance table\n",
    "                if iter % (num_iterations//4) == 0 and iter != num_iterations//4*4:\n",
    "                    add_row(training_table, f\" \", f\"{iter}\",f\"{loss.item():.6f}\", \" \")\n",
    "                    if IS_NOTEBOOK:\n",
    "                        display(Javascript(f\"\"\"addRow(\"\", \"{iter}\", \"{loss.item():.6f}\", \"\");\"\"\"))\n",
    "                    else:\n",
    "                        print_row(training_table)\n",
    "                elif iter == 1:\n",
    "                    add_row(training_table, f\"{epoch}/{NUM_EPOCHS}\", f\"{iter}/{num_iterations}\",f\"{loss.item():.6f}\", \" \")\n",
    "                    if IS_NOTEBOOK:\n",
    "                        display(Javascript(f\"\"\"addRow(\"<b>{epoch}/{NUM_EPOCHS}\", \"{iter}/{num_iterations}\", \"{loss.item():.6f}\", \"\");\"\"\"))\n",
    "                    else:\n",
    "                        print_row(training_table)\n",
    "                        \n",
    "                # -------------------------------------------------------------\n",
    "                # Update running loss and progress bar\n",
    "                running_loss += loss.item() # acculumate loss for epoch\n",
    "                tepoch.set_postfix(loss=loss.item()); tepoch.update(1)\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Update the performance table\n",
    "        add_row(training_table, f\" \", f\"{iter}\",f\"{loss.item():.6f}\", f\"{avg_train_loss:6f}\")\n",
    "        if IS_NOTEBOOK:\n",
    "            display(Javascript(f\"\"\"addRow(\"\", \"{iter}\", \"{loss.item():.6f}\", \"<b>{avg_train_loss:.6f}\");\"\"\"))\n",
    "        else:\n",
    "            print_row(training_table)\n",
    "\n",
    "        # VALIDATION\n",
    "        if val_loader:\n",
    "            val_loss = validate_model(model, val_loader, loss_fn)\n",
    "            val_losses.append(val_loss)\n",
    "            current_lr = scheduler.get_last_lr()\n",
    "            scheduler.step(val_loss)    # Adjust learning rate based on validation loss\n",
    "            if current_lr != scheduler.get_last_lr(): print(f\"LR: {scheduler.get_last_lr()}\")\n",
    "                \n",
    "            # Update the performance table\n",
    "            add_row(training_table, f\" \", f\"Validation Loss:\",f\"{val_loss:.6f}\", f\"\")\n",
    "            if IS_NOTEBOOK:\n",
    "                display(Javascript(f\"\"\"addRow(\"<b>Val\", \"Validation Loss:\", \"<b>{val_loss:.4f}\", \"\");\"\"\"))\n",
    "            else:\n",
    "                print_row(training_table)\n",
    "\n",
    "    print(f\"{'-'*60}\\nTraining Completed.\\tExecution Time: \", f\"{(time.perf_counter() - start_time):.2f}\", f\"s\\n\")\n",
    "\n",
    "    return {\"train_losses\": train_losses, \"val_losses\": val_losses, \"epoch\": epoch, \"training_table\": training_table, \n",
    "            \"model\": model, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(),\"loss_fn\": loss_fn}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "NETWORK TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started.\tProcess ID: 1845482 \n",
      "------------------------------------------------------------\n",
      "Model: LSTM1\t\tParameters on device: cuda:0\n",
      "------------------------------------------------------------\n",
      "Train/Batch size:\t10106504 / 2048\n",
      "Loss:\t\t\tHuberLoss()\n",
      "Optimizer:\t\tAdamW\n",
      "LR:\t\t\t0.005\n",
      "Weight Decay:\t\t0.0001\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"scrollable_table\" style=\"height: 300px; overflow-y: scroll;\">\n",
       "    <table id=\"training_table\" style=\"width:60%; border-collapse: collapse;\">\n",
       "        <thead style=\"position: sticky; top: 0; z-index: 1;\">\n",
       "            <tr>\n",
       "                <th style=\"font-weight:bold; width:15%; text-align:left; padding: 10px; background-color: #404040;\">Epoch</th>\n",
       "                <th style=\"font-weight:bold; width:25%; text-align:left; padding: 10px; background-color: #404040;\">Iteration</th>\n",
       "                <th style=\"font-weight:bold; width:30%; text-align:left; padding: 10px; background-color: #404040;\">Batch Loss</th>\n",
       "                <th style=\"font-weight:bold; width:30%; text-align:left; padding: 10px; background-color: #404040;\">Train Loss</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        </tbody>\n",
       "    </table>\n",
       "    <script>\n",
       "        function addRow(epoch, step, loss, running_loss) {\n",
       "            var table = document.getElementById(\"training_table\").getElementsByTagName('tbody')[0];\n",
       "            var row = table.insertRow(-1);\n",
       "            var cell1 = row.insertCell(0);\n",
       "            var cell2 = row.insertCell(1);\n",
       "            var cell3 = row.insertCell(2);\n",
       "            var cell4 = row.insertCell(3);\n",
       "            cell1.style.textAlign = \"left\";\n",
       "            cell2.style.textAlign = \"left\";\n",
       "            cell3.style.textAlign = \"left\";\n",
       "            cell4.style.textAlign = \"left\";\n",
       "            cell1.innerHTML = epoch;\n",
       "            cell2.innerHTML = step;\n",
       "            cell3.innerHTML = loss;\n",
       "            cell4.innerHTML = running_loss;\n",
       "            var scrollableDiv = document.getElementById(\"scrollable_table\");\n",
       "            scrollableDiv.scrollTop = scrollableDiv.scrollHeight;\n",
       "        }\n",
       "    </script>\n",
       "    </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54ef31d82814a3981d70d8ec2f843d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4935 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "addRow(\"<b>1/20\", \"1/4935\", \"0.115952\", \"\");",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# NETWORK TRAINING -----------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, loss_fn, train_loader, val_loader, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m header_printed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mnum_iterations, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtepoch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtepoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# -------------------------------------------------------------\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move data to the GPU\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m, in \u001b[0;36mTripDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(target):\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[i][index], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),  \u001b[38;5;66;03m# Add time dimension\u001b[39;00m\n\u001b[1;32m     46\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(target[index], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     47\u001b[0m         )\n\u001b[0;32m---> 48\u001b[0m     index \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(target)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex out of range\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NETWORK TRAINING -----------------------------------------------------------------\n",
    "trained = train_model(\n",
    "    model = model, \n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler, \n",
    "    loss_fn = criterion, \n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL  -----------------------------------------------------------------\n",
    "# create unique model name\n",
    "model_name = f'{model.__class__.__name__}_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}'\n",
    "model_destination_path = Path(pth_folder, model_name + \".pth\")\n",
    "\n",
    "# add the splitted data subset information to save dict\n",
    "trained['subset_files'] = subset_files\n",
    "# save the model & print info\n",
    "torch.save(trained, model_destination_path)\n",
    "print(f\"Model saved to:\\t {model_destination_path}\\n{'-'*60}\\nSize: {os.path.getsize(model_destination_path) / 1e6:.2f} MB\\n{'-'*60}\")\n",
    "if os.path.getsize(model_destination_path) > 100 * 1024**2: print(\"--> Warning: The saved model size exceeds 100MB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL AT CHECKPOINT -----------------------------------------------------------------\n",
    "# Select Model to Load:\n",
    "model_destination_path = Path(pth_folder, \"LSTM1_241106_202042.pth\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "checkpoint = torch.load(model_destination_path, weights_only=False)\n",
    "for key in [\"model\", \"loss_fn\", \"training_table\", \"train_losses\", \"val_losses\", \"epoch\"]: globals()[key] = checkpoint[key]\n",
    "\n",
    "# configure model and optimizer:\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval(); # set model to evaluation mode for inference\n",
    "print(f\"Model loaded from:\\t {model_destination_path}\\n{'-'*60}\")\n",
    "print(f\"Model: {model.__class__.__name__}\\t\\tParameters on device: {next(model.parameters()).device}\\n{'-'*60}\")\n",
    "\n",
    "# get file list of test subset\n",
    "test_files = checkpoint[\"subset_files\"][\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and its parameters to the CPU\n",
    "model.to('cuda:0')\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUME TRAINING -----------------------------------------------------------------\n",
    "resume = True\n",
    "if resume: \n",
    "    NUM_EPOCHS += 20 # train for 2 more epochs\n",
    "    trained = train_model(\n",
    "        model = model, \n",
    "        optimizer = optimizer, \n",
    "        scheduler = scheduler,\n",
    "        loss_fn = loss_fn, \n",
    "        train_loader = train_loader,\n",
    "        val_loader = val_loader,\n",
    "        state = checkpoint if resume else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DataFrame of training metrics:\n",
    "training_df = pd.DataFrame(training_table, columns=[\"Epoch\", \"Iteration\", \"Batch Loss\", \"Train Loss\"])\n",
    "# Extract the 'Train Loss' column and compare with the train_losses list\n",
    "train_loss_column = training_df['Train Loss'].replace(['',' '], np.nan).dropna().astype(float).values\n",
    "if any(abs(train_loss_column - train_losses) > 1e-3): \n",
    "    print(\"Extracted and original Train Losses are not equal. Please check metrics table.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# plot training performance:\n",
    "fig, ax1 = plt.subplots(figsize=(8,3))\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_xticks(range(1, NUM_EPOCHS + 1))\n",
    "\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='train_loss')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), val_losses, label='val_loss')\n",
    "plt.yscale('log'); fig.tight_layout(); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "EVALUATION / POST-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION -----------------------------------------------------------------\n",
    "model.eval() # set model to evaluation mode\n",
    "test_loss = 0\n",
    "all_outputs, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Collect all outputs and targets\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "        all_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Inverse-transform on all outputs and targets for evaluation\n",
    "scaled_outputs = target_scaler.inverse_transform(np.concatenate(all_outputs, axis=0).reshape(-1, 1))\n",
    "scaled_targets = target_scaler.inverse_transform(np.concatenate(all_targets, axis=0).reshape(-1, 1))\n",
    "# Calculate RMSE\n",
    "rmse = root_mean_squared_error(scaled_targets, scaled_outputs)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss:  {test_loss:.4f}\")\n",
    "print(f\"Iterations: {iter}/{math.floor(len(test_loader.dataset) / test_loader.batch_size)}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TripDataset(random.sample(test_files, 1), scaler, target_scaler)\n",
    "test_loader_2 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_2:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        y_pred.append(torch.mean(outputs).item())\n",
    "\n",
    "y_pred = target_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
    "y_true = target_scaler.inverse_transform(np.array(test_loader_2.dataset.targets[0]).reshape(-1, 1))\n",
    "\n",
    "###############################################\n",
    "print(f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\")\n",
    "\n",
    "# PLOT PREDICTION -----------------------------------------------------------------\n",
    "plt.figure(figsize=(18,4)); plt.xlabel('Time in s'); plt.ylabel('Battery Energy in kWh'); plt.title('Time-Series Prediction')\n",
    "plt.plot(y_true, label='Actual Data') # actual plot\n",
    "plt.plot(np.arange(0, len(y_true), 1), y_pred, label='Predicted Data') # predicted plot\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize=(18,4)); plt.xlabel('Time in s'); plt.ylabel('Battery Energy in kWh'); plt.title('Time-Series Prediction (Smoothed)')\n",
    "plt.plot(savgol_filter(y_true.flatten(), window_length=60, polyorder=3), label='Actual Data (Smoothed)') # actual plot\n",
    "plt.plot(np.arange(0, len(y_true), 1), savgol_filter(y_pred.flatten(), window_length=60, polyorder=3), label='Predicted Data (Smoothed)') # predicted plot\n",
    "plt.legend();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
