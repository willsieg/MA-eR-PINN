{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfaf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset -f -s\n",
    "%matplotlib inline\n",
    "'''------------------------------------------------------------------\n",
    "MA-eR-PINN: eRange Prediction using Physics-Informed Neural Networks\n",
    "---------------------------------------------------------------------\n",
    "Version: V2.0      Modified: 12.01.2025        William Siegle\n",
    "---------------------------------------------------------------------\n",
    "PTRAIN - Standard Pipeline Framework for Training the PINN\n",
    "+ OPTUNA - Hyperparameter Optimization using Optuna\n",
    "------------------------------------------------------------------''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55581895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA-eR-PINN: CONFIGURATION FILE -------------------------------------------------\n",
    "from pathlib import Path\n",
    "CONFIG = {\n",
    "    # SYSTEM: ---------------------------------------------------------------------\n",
    "    \"GPU_SELECT\":       0,\n",
    "    \"ROOT\":             Path('../..').resolve(),\n",
    "    \"INPUT_LOCATION\":   Path(\"TripSequences\", \"trips_processed_pinn_2\"), \n",
    "    \"OUTPUT_LOCATION\":  Path(\"src\", \"models\", \"pth\"),\n",
    "    \"SEED\"  :           20,\n",
    "    \"MIXED_PRECISION\":  True,\n",
    "\n",
    "    # DATA PREPROCESSING: ---------------------------------------------------------\n",
    "    \"TRAIN_VAL_TEST\":   [0.8, 0.1, 0.1], # [train, val, test splits]\n",
    "    \"MAX_FILES\":        None, # None: all files\n",
    "    \"MIN_SEQ_LENGTH\":   3600, # minimum sequence length in s to be included in DataSets\n",
    "    \"SCALERS\":          {'feature_scaler': 'MinMaxScaler()','target_scaler': 'MinMaxScaler()','prior_scaler': 'MinMaxScaler()'},\n",
    "\n",
    "    # FEATURES: -------------------------------------------------------------------\n",
    "    \"FEATURES\":         ['accelpdlposn_cval','actdrvtrnpwrprc_cval','actualdcvoltage_pti1','actualspeed_pti1','actualtorque_pti1',\n",
    "                        'airtempinsd_cval_hvac','airtempinsd_rq','airtempoutsd_cval_cpc','altitude_cval_ippc','brc_stat_brc1','brktempra_cval',\n",
    "                        'bs_brk_cval','currpwr_contendrnbrkresist_cval','elcomp_pwrcons_cval','epto_pwr_cval','hv_bat_dc_momvolt_cval_bms1',\n",
    "                        'hv_batavcelltemp_cval_bms1','hv_batcurr_cval_bms1','hv_batisores_cval_e2e','hv_batmaxchrgpwrlim_cval_1',\n",
    "                        'hv_batmaxdischrgpwrlim_cval_1','hv_curr_cval_dcl1','hv_dclink_volt_cval_dcl1','hv_ptc_cabin1_pwr_cval','hv_pwr_cval_dcl1',\n",
    "                        'lv_convpwr_cval_dcl1','maxrecuppwrprc_cval','maxtracpwrpct_cval','motortemperature_pti1','powerstagetemperature_pti1',\n",
    "                        'rmsmotorcurrent_pti1','roadgrad_cval_pt','selgr_rq_pt','start_soc','txoiltemp_cval_tcm','vehspd_cval_cpc','vehweight_cval_pt'],                 \n",
    "    \"TARGETS\":          ['hv_bat_soc_cval_bms1'],\n",
    "    \"PRIORS\":           ['emot_soc_pred'],  \n",
    "\n",
    "    # MODEL: -----------------------------------------------------------------------\n",
    "    \"HIDDEN_SIZE\":      12,    # features in the hidden state h\n",
    "    \"NUM_LAYERS\":       3,      # recurrent layers for stacked LSTMs. Default: 1\n",
    "    \"DROPOUT\":          0.35,   # usually: [0.2 - 0.5]\n",
    "    \n",
    "    # TRAINING & OPTIMIZER: --------------------------------------------------------\n",
    "    \"NUM_EPOCHS\":       10,\n",
    "    \"BATCH_SIZE\":       32,         # [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    \"LEARNING_RATE\":    1e-3,       # 0.001 lr\n",
    "    \"WEIGHT_DECAY\":     1e-3,       # weight decay coefficient (default: 1e-2)\n",
    "    \"MOMENTUM_SGD\":     0.1,        # (default: 0.0)\n",
    "    \"OPTIMIZER\":        'adamw',    # ('adam', 'sgd', 'adamw')\n",
    "    \"WEIGHT_INIT_TYPE\": 'he',  # ('he', 'normal', 'default')\n",
    "    \"CLIP_GRAD\":        2.0,        # default: None\n",
    "    \"LRSCHEDULER\":      \"torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\",  # constant LR for 1.0 as multiplicative factor\n",
    "                        # torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 3, factor = 0.5, min_lr = 1e-7)\n",
    "\n",
    "    # LOSS FUNCTION: ---------------------------------------------------------------\n",
    "    \"CRITERION\":        \"nn.SmoothL1Loss()\", # ['nn.MSELoss()', 'nn.L1Loss()', 'nn.SmoothL1Loss()', 'nn.HuberLoss()', 'MASE()']\n",
    "    \"P_LOSS_FACTOR\":    0.5, # Physics loss factor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc61569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA: SEARCH SPACE ---------------------------------------------------\n",
    "N_TRIALS = 50\n",
    "\n",
    "global search_space, search_space_NewData\n",
    "search_space = {\n",
    "    # MODEL: -----------------------------------------------------------------------\n",
    "    'HIDDEN_SIZE': ('int', 10, 100, 10),\n",
    "    'NUM_LAYERS': ('int', 1, 5, 1),\n",
    "    'DROPOUT': ('float', 0.0, 0.6, 0.05),\n",
    "    'CLIP_GRAD': ('categorical', (None, 0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0)),\n",
    "    'WEIGHT_INIT_TYPE': ('categorical', ('he', 'normal', 'default')),\n",
    "\n",
    "    # TRAINING & OPTIMIZER: --------------------------------------------------------\n",
    "    'OPTIMIZER': ('categorical', ('adam', 'adamw')),\n",
    "    #'NUM_EPOCHS': ('int', 5, 10, 1),\n",
    "    'LEARNING_RATE': ('categorical', (5e-5, 1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 3e-3, 5e-3, 8e-3, 1e-2, 2e-2, 5e-2)),\n",
    "    'WEIGHT_DECAY': ('categorical', (0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2)),\n",
    "    #'MOMENTUM_SGD': ('float', 0.0, 0.9, 0.1),\n",
    "\n",
    "    #'P_LOSS_FACTOR': ('float', 0.05, 1.0, 0.05)\n",
    "}\n",
    "\n",
    "search_space_NewData = {\n",
    "    # DATA PREPROCESSING: ---------------------------------------------------------\n",
    "    \"MIN_SEQ_LENGTH\": ('int', 300, 3600, 300),\n",
    "    'BATCH_SIZE': ('categorical', (4, 8, 16, 32, 64, 128)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b8868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION MODULES ----------------------------------------------------------------   \n",
    "\n",
    "def loss_fn_PINN_3(output, target, prior):\n",
    "    l_p = P_LOSS_FACTOR\n",
    "    y_pred = output; y_true = target; y_phys = prior\n",
    "    total_loss = F.mse_loss(y_true, (l_p * y_phys + (1 - l_p) * y_pred), reduction='mean')\n",
    "    return total_loss\n",
    "\n",
    "global LOSS_FN\n",
    "LOSS_FN = loss_fn_PINN_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c4680",
   "metadata": {},
   "source": [
    "___\n",
    "SETUP: Locate devices & system folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  ---------------------------\n",
    "import sys, os\n",
    "for key in CONFIG: globals()[key] = CONFIG[key]\n",
    "print(ROOT)\n",
    "if 'ROOT' not in globals(): ROOT = Path('../..').resolve()\n",
    "sys.path.append(os.path.abspath(ROOT))\n",
    "\n",
    "# INTERNAL MODULE IMPORTS ----------------------------------------------------------------\n",
    "from src.__init__ import *\n",
    "from src.utils.data_utils import *\n",
    "from src.utils.preprocess_utils import *\n",
    "from src.utils.eval_utils import *\n",
    "from src.utils.Trainers import *\n",
    "from src.models.lstm_models import *\n",
    "\n",
    "# SETUP ENVIRONMENT ---------------------------------------------------------------------\n",
    "DATA_PATH, IS_NOTEBOOK, DEVICE, LOG_FILE_NAME, TS = setup_environment(CONFIG, ROOT, SEED, GPU_SELECT)\n",
    "if not IS_NOTEBOOK: output_file = open(f\"{LOG_FILE_NAME}\", \"w\"); sys.stdout = Tee(sys.stdout, output_file); sys.stderr = Tee(sys.stderr, output_file)\n",
    "\n",
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, INPUT_LOCATION) # Trip parquet files\n",
    "pth_folder = Path(ROOT, OUTPUT_LOCATION, f\"{TS}\")\n",
    "pth_folder.mkdir(parents=True, exist_ok=True)\n",
    "files, trip_lengths, indices_by_length, sorted_trip_lengths, all_signals = prepare_data(input_folder, pth_folder, MAX_FILES, MIN_SEQ_LENGTH, ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1963d1",
   "metadata": {},
   "source": [
    "___\n",
    "DATA SELECTION & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb951c30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# INPUT & TARGET SPECIFICATION ---------------------------------------------------\n",
    "# these signals are required for the physical Model calculation:\n",
    "base_signals = [\"signal_time\", \"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \n",
    "                \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "\n",
    "# these signals have to be dropped (from Features) in order for appropriate training:\n",
    "columns_to_drop = [\"signal_time\",                       # works as index\n",
    "                    \"hirestotalvehdist_cval_icuc\",      # starts from 0, obtained by speed integration\n",
    "                    \"latitude_cval_ippc\",               # only GPS \n",
    "                    \"longitude_cval_ippc\",              # only GPS\n",
    "                    \"hv_batpwr_cval_bms1\",              # directly related to target (soc_gradient)\n",
    "                    \"hv_batmomavldischrgen_cval_1\",     # indirect target 1 in kWh\n",
    "                    \"hv_bat_soc_cval_bms1\",              # indirect target 2 in %SoC\n",
    "                    \"soc_gradient\",                     # actual target signal   \n",
    "                    \"emot_pwr_cval\",                    # replaced as physical prior for PINN\n",
    "                    \"emot_pwr_pred\",                    # actual physical prior for PINN\n",
    "                    ]\n",
    "\n",
    "# Ensure no element of \"columns_to_drop\" is included in \"FEATURES\"\n",
    "assert not any(col in FEATURES for col in columns_to_drop), \"Some columns to drop are still in FEATURES\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "selection_1 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", 'roadgrad_cval_pt', \"vehweight_cval_pt\", \"accelpdlposn_cval\", \"bs_brk_cval\", \"elcomp_pwrcons_cval\",\n",
    "               \"epto_pwr_cval\", \"motortemperature_pti1\", \"powerstagetemperature_pti1\", 'airtempinsd_cval_hvac', 'brktempra_cval', 'selgr_rq_pt']\n",
    "selection_2 = [\"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\", \"roadgrad_cval_pt\"]\n",
    "selection_3 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"vehweight_cval_pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SELECTION & SCALING ----------------------------------------------------------------------------\n",
    "INPUT_COLUMNS = FEATURES; TARGET_COLUMN = TARGETS; PRIOR_COLUMN = PRIORS\n",
    "print(f\"{'-'*60}\\nInput Signals:\\t{len(FEATURES)}\\nTarget Signals:\\t{len(TARGETS)}\\nPhysical Prior Signals:\\t{len(PRIORS)}\\n{'-'*60}\")\n",
    "scaler, target_scaler, prior_scaler = eval(SCALERS['feature_scaler']), eval(SCALERS['target_scaler']), eval(SCALERS['prior_scaler'])\n",
    "\n",
    "# DATA SET SPLITTING AND SORTING ----------------------------------------------------------------\n",
    "train_subset, val_subset, test_subset = random_split(files, TRAIN_VAL_TEST)\n",
    "\n",
    "# DATALOADER SETTINGS ------------------------------------------------------------------\n",
    "dataloader_settings = {'batch_size': 1, 'shuffle': True, 'collate_fn': collate_fn_PINN, 'num_workers': 8,\n",
    " 'prefetch_factor': 4, 'persistent_workers': True, 'pin_memory': False if DEVICE.type == 'cpu' else True}\n",
    "\n",
    "# PREPARE TRAIN, VAL & TEST DATALOADERS  ------------------------------------------------------------\n",
    "train_subset, train_dataset, train_dataset_batches, train_loader = prepare_dataloader_PINN(train_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, fit=True, drop_last=True)\n",
    "\n",
    "val_subset, val_dataset, val_dataset_batches, val_loader = prepare_dataloader_PINN(val_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, drop_last=False)\n",
    "\n",
    "test_subset, test_dataset, test_dataset_batches, test_loader = prepare_dataloader_PINN(test_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, drop_last=False)\n",
    "\n",
    "# print dataset info\n",
    "subset_files = print_dataset_sizes(train_dataset, val_dataset, test_dataset, train_subset, val_subset, test_subset, files)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Load dataloaders instead\n",
    "#train_loader = torch.load('train_loader.pth')\n",
    "#val_loader = torch.load('val_loader.pth')\n",
    "#test_loader = torch.load('test_loader.pth')\n",
    "\n",
    "# optional visualizations of padding preprocessing:\n",
    "if IS_NOTEBOOK and False: \n",
    "    check_batch_PINN(train_loader)\n",
    "    visualize_padding(BATCH_SIZE, trip_lengths, sorted_trip_lengths, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3fe74",
   "metadata": {},
   "source": [
    "___\n",
    "MODEL & TRAINING CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ef486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM NETWORK -----------------------------------------------------------------------\n",
    "\n",
    "class LSTM1_packed_old_version(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, device=DEVICE):\n",
    "        super(LSTM1_packed_old_version, self).__init__()\n",
    "\n",
    "        self.input_size = input_size    # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.num_layers = num_layers    # number of layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # LSTM CELL --------------------------------\n",
    "        self.lstm = nn.LSTM(self.input_size,self.hidden_size,self.num_layers,batch_first=True,dropout=self.dropout,device=device)\n",
    "\n",
    "        # LAYERS -----------------------------------\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, packed_input, batch_size=None):\n",
    "        packed_out, _ = self.lstm(packed_input)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.dropout_layer(out)  # dropout\n",
    "        out = self.fc1(out)  # fully connected layer 1\n",
    "        out = self.bn1(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc2(out)  # fully connected layer 2\n",
    "        return out\n",
    "\n",
    "    # Define the weight initialization function for LSTM\n",
    "    def initialize_weights_lstm(self, init_type):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name or 'weight_hh' in name:\n",
    "                if init_type == 'he': nn.init.kaiming_uniform_(param.data, nonlinearity='relu')     # HE INIT\n",
    "                elif init_type == 'normal': nn.init.normal_(param.data, mean=0.0, std=0.02)         # NORMAL INIT\n",
    "                elif init_type == 'default': continue                                               # TORCH DEFAULT INIT\n",
    "            elif 'bias' in name and init_type != 'default': nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0d8b3",
   "metadata": {},
   "source": [
    "___\n",
    "OPTUNA: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9c4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA: OBJECTIVE ---------------------------------------------------\n",
    "def objective(trial, LOSS_FN):\n",
    "\n",
    "    # OPTUNA: CREATE TRIAL OBJECTS ---------------------------------------------------\n",
    "    optuna_params = {}\n",
    "    for param, (suggest_type, *args) in search_space.items():\n",
    "        if suggest_type == 'int': optuna_params[param.lower()] = trial.suggest_int(param, *args[:2], step=args[2])\n",
    "        elif suggest_type == 'float': optuna_params[param.lower()] = trial.suggest_float(param, *args[:2], step=args[2])\n",
    "        elif suggest_type == 'categorical': optuna_params[param.lower()] = trial.suggest_categorical(param, *args)\n",
    "    \n",
    "    # Update CONFIG with suggested hyperparameters\n",
    "    for param in optuna_params.keys(): CONFIG[param.upper()] = optuna_params[param]\n",
    "    for key in CONFIG: globals()[key] = CONFIG[key]\n",
    "    # -----------------------------------------------------------------------------------\n",
    "\n",
    "    # TRAINING_CODE: -----------------------------------------------------------------\n",
    "    # INSTANTIATE MODEL AND APPLY WEIGHT INITIALIZATION --------------------\n",
    "    model = LSTM1_packed_old_version(len(INPUT_COLUMNS), HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "    model.initialize_weights_lstm(WEIGHT_INIT_TYPE)\n",
    "    #print_info(model)\n",
    "    \n",
    "    # SET OPTIMIIZER, SCHEDULER AND LOSS MODULES ---------------------------\n",
    "    if OPTIMIZER=='adam': optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    elif OPTIMIZER=='adamw': optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "    elif OPTIMIZER=='sgd': optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM_SGD, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def lr_lambda(epoch): return 1.0\n",
    "    scheduler = eval(LRSCHEDULER); criterion = eval(CRITERION)\n",
    "\n",
    "    # TRAIN -----------------------------------------------------------------\n",
    "    TRAINER = PTrainer_PINN(\n",
    "        model = model, \n",
    "        optimizer = optimizer, \n",
    "        scheduler = scheduler,\n",
    "        loss_fn = LOSS_FN, \n",
    "        train_loader = train_loader, \n",
    "        val_loader = val_loader, \n",
    "        test_loader = test_loader, \n",
    "        num_epochs = NUM_EPOCHS, \n",
    "        device = DEVICE, \n",
    "        is_notebook = IS_NOTEBOOK, \n",
    "        use_mixed_precision = MIXED_PRECISION, \n",
    "        clip_value = CLIP_GRAD, \n",
    "        log_file = Path(LOG_FILE_NAME).with_name(f\"{TS}_Trial{trial.number}.txt\"))\n",
    "\n",
    "    RESULTS = TRAINER.train_model()\n",
    "\n",
    "    # PLOT RESULTS AND SAVE OPERATIONS ----------------------------------------------\n",
    "    print(f\"Trial {trial.number}\")\n",
    "    plot_training_performance(RESULTS)\n",
    "    \n",
    "    # TRAIN/VAL Loss:\n",
    "    val_loss = RESULTS['val_losses'][-1]\n",
    "    train_loss = RESULTS['train_losses'][-1]\n",
    "\n",
    "    # EVALUATE -----------------------------------------------------------------\n",
    "    test_loss, all_outputs, all_targets, all_priors, all_original_lengths = TRAINER.evaluate_model()\n",
    "    # back-transform:\n",
    "    scaled_outputs = [target_scaler.inverse_transform(output_sequence.reshape(1, -1)).squeeze() for output_sequence in all_outputs]\n",
    "    scaled_targets = [target_scaler.inverse_transform(target_sequence.reshape(1, -1)).squeeze() for target_sequence in all_targets]\n",
    "    scaled_priors = [prior_scaler.inverse_transform(prior_sequence.reshape(1, -1)).squeeze() for prior_sequence in all_priors]\n",
    "    # concatenate:\n",
    "    all_y_true, all_y_pred, all_y_phys = np.concatenate(scaled_targets), np.concatenate(scaled_outputs), np.concatenate(scaled_priors)\n",
    "    # calculate evaluation metrics\n",
    "    print(f\"Test Loss:\\t\\t{test_loss:.6f}\")\n",
    "    metrics = calculate_metrics(all_y_true, all_y_pred) # [rmse, mae, std_dev, mape, r2, max_error]\n",
    "\n",
    "    rmse = metrics[\"rmse\"]\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA: STUDY -------------------------------------------------------------------\n",
    "study = optuna.create_study(direction='minimize', sampler = optuna.samplers.TPESampler())    # TPESampler, RandomSampler, GridSampler, CmaEsSampler, NSGAIISampler\n",
    "study.optimize(lambda trial: objective(trial, LOSS_FN), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc2065",
   "metadata": {},
   "source": [
    "___\n",
    "SAVE CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best trial config to a text file\n",
    "best_trial = study.best_trial\n",
    "best_trial_dict = {'params': best_trial.params, 'value': best_trial.value}\n",
    "with open(Path(LOG_FILE_NAME).with_name(f\"{TS}_BEST_IS_{best_trial.number}.txt\"), 'w') as f: json.dump(best_trial_dict, f, indent=4)\n",
    "print(\"Best Trial: \", best_trial.number); print(\"Best hyperparameters: \", study.best_params)\n",
    "trials_df = study.trials_dataframe()\n",
    "print(tabulate(trials_df, headers='keys', tablefmt='psql'))\n",
    "trials_df.to_csv('trials_overview.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sieglew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
