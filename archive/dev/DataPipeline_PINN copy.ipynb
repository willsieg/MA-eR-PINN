{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------------------------------------------------------------\n",
    "---------------------------------------------------------------------\n",
    "LSTM Training\n",
    "---------------------------------------------------------------------\n",
    "Version: V1.7       Modified: 06.11.2024        William Siegle\n",
    "---------------------------------------------------------------------\n",
    "notebook can be converted to python script using: \n",
    "(python -m) jupytext --to py FILENAME.ipynb\n",
    "------------------------------------------------------------------'''\n",
    "#import pathlib\n",
    "from pathlib import Path, WindowsPath, PosixPath\n",
    "\n",
    "\n",
    "# SETTINGS ------------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    # SYSTEM: ---------------------------------------------------------------------\n",
    "    \"GPU_SELECT\":       0, # {0,1,2,3, None: CPU only}\n",
    "    \"ROOT\":             Path('../..').resolve(),\n",
    "    \"INPUT_LOCATION\":   Path(\"TripSequences\", \"trips_processed_pinn_2\"), \n",
    "    \"OUTPUT_LOCATION\":  Path(\"src\", \"models\", \"pth\"),\n",
    "    \"SEED\"  :           1,\n",
    "    \"PLOT_ACTIVE\":      True,\n",
    "\n",
    "    # DATA PREPROCESSING: ---------------------------------------------------------\n",
    "    \"TRAIN_VAL_TEST\":   [0.8, 0.18, 0.02], # [train, val, test splits]\n",
    "    \"MAX_FILES\":        None, # None: all files\n",
    "    \"SCALERS\":          {'feature_scaler': 'MaxAbsScaler()',\n",
    "                        'target_scaler': 'MinMaxScaler(feature_range=(0, 1))',\n",
    "                        'prior_scaler': 'MinMaxScaler(feature_range=(0, 1))'},\n",
    "    \"MIN_SEQ_LENGTH\":   3600, # minimum sequence length in s to be included in DataSets\n",
    "\n",
    "    # FEATURES: -------------------------------------------------------------------\n",
    "    \"FEATURES\":         ['actdrvtrnpwrprc_cval','altitude_cval_ippc','motortemperature_pti1',\n",
    "                        'brktempra_cval','hv_batmaxdischrgpwrlim_cval_1',\n",
    "                        'actualtorque_pti1','powerstagetemperature_pti1',\n",
    "                        'accelpdlposn_cval','airtempoutsd_cval_cpc','airtempinsd_cval_hvac','actualdcvoltage_pti1','hv_batavcelltemp_cval_bms1',\n",
    "                        'epto_pwr_cval','maxtracpwrpct_cval','airtempinsd_rq','bs_brk_cval','brc_stat_brc1','vehspd_cval_cpc',\n",
    "                        'vehweight_cval_pt','actualspeed_pti1','selgr_rq_pt',\n",
    "                        'rmsmotorcurrent_pti1','roadgrad_cval_pt','currpwr_contendrnbrkresist_cval','elcomp_pwrcons_cval',\n",
    "                        'hv_ptc_cabin1_pwr_cval','maxrecuppwrprc_cval','txoiltemp_cval_tcm', 'start_soc',\n",
    "                        'hv_curr_cval_dcl1','hv_dclink_volt_cval_dcl1','hv_pwr_cval_dcl1','lv_convpwr_cval_dcl1','hv_batmaxchrgpwrlim_cval_1'],\n",
    "                        \n",
    "    \"TARGETS\":          ['hv_bat_soc_cval_bms1'],\n",
    "    \"PRIORS\":           ['emot_soc_pred'],  \n",
    "\n",
    "    # MODEL: -----------------------------------------------------------------------\n",
    "    \"HIDDEN_SIZE\":      512,    # features in the hidden state h\n",
    "    \"NUM_LAYERS\":       3,      # recurrent layers for stacked LSTMs. Default: 1\n",
    "    \"DROPOUT\":          0.5,\n",
    "    \n",
    "    # TRAINING & OPTIMIZER: --------------------------------------------------------\n",
    "    \"NUM_EPOCHS\":       4,\n",
    "    \"BATCH_SIZE\":       32,   # [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "    \"LEARNING_RATE\":    2e-3,   # 0.001 lr\n",
    "    \"OPTIMIZER\":        \"torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = 1e-3)\",      \n",
    "                        # weight_decay = 1e-4     # weight decay coefficient (default: 1e-2)\n",
    "                        # betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "                        # eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"LRSCHEDULER\":      \"torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 3, factor = 0.5, min_lr = 1e-7)\",\n",
    "\n",
    "    # LOSS FUNCTION: ---------------------------------------------------------------\n",
    "    \"CRITERION\":        \"nn.SmoothL1Loss()\", # ['nn.MSELoss()', 'nn.L1Loss()', 'nn.SmoothL1Loss()', 'nn.HuberLoss()', 'MASE()']\n",
    "    \"LOSS_FN\":          \"F.mse_loss(output, target)\", # ['F.mse_loss(output, target)', 'F.l1_loss(output, target)', 'F.smooth_l1_loss(output, target)', 'F.huber_loss(output, target)', 'F.mase_loss(output, target)']\n",
    "\n",
    "    # SAVE & LOAD: -----------------------------------------------------------------\n",
    "    \"MODE\":             \"train_mode\", # ['train_mode', 'test_mode']\n",
    "    \"TRAIN_LOG\":        \"test2.txt\",\n",
    "\n",
    "}\n",
    "for key in CONFIG: globals()[key] = CONFIG[key];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "LOCATE DEVICES & SYSTEM FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Directories:\n",
      "  /home/sieglew/MA-eR-PINN:\t\t\tproject, test, data, src, .git, archive\n",
      "  /home/sieglew/MA-Data:\t\t\t__pycache__, trips_processed_resampled, final, y_true, processed, TripSequences, trips_processed_pickles, final_2, trips_processed_final\n",
      "------------------------------------------------------------\n",
      "Running in notebook mode\n",
      "CONFIG Dictionary:\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "     Parameter        Value\n",
      "--  ---------------  ------------------------------------------------------------------------------------------------------------\n",
      "0   GPU_SELECT       0\n",
      "1   ROOT             /home/sieglew/MA-eR-PINN\n",
      "2   INPUT_LOCATION   TripSequences/trips_processed_pinn_2\n",
      "3   OUTPUT_LOCATION  src/models/pth\n",
      "4   SEED             1\n",
      "5   PLOT_ACTIVE      True\n",
      "6   TRAIN_VAL_TEST   [0.8, 0.18, 0.02]\n",
      "7   MAX_FILES        None\n",
      "8   SCALERS          {'feature_scaler': 'MaxAbsScaler()',\n",
      "                      'target_scaler': 'MinMaxScaler(feature_range=(0,\n",
      "                      1))',\n",
      "                      'prior_scaler': 'MinMaxScaler(feature_range=(0,\n",
      "                      1))'}\n",
      "9   MIN_SEQ_LENGTH   3600\n",
      "10  FEATURES         ['actdrvtrnpwrprc_cval',\n",
      "                      'altitude_cval_ippc',\n",
      "                      'motortemperature_pti1',\n",
      "                      'brktempra_cval',\n",
      "                      'hv_batmaxdischrgpwrlim_cval_1',\n",
      "                      'actualtorque_pti1',\n",
      "                      'powerstagetemperature_pti1',\n",
      "                      'accelpdlposn_cval',\n",
      "                      'airtempoutsd_cval_cpc',\n",
      "                      'airtempinsd_cval_hvac',\n",
      "                      'actualdcvoltage_pti1',\n",
      "                      'hv_batavcelltemp_cval_bms1',\n",
      "                      'epto_pwr_cval',\n",
      "                      'maxtracpwrpct_cval',\n",
      "                      'airtempinsd_rq',\n",
      "                      'bs_brk_cval',\n",
      "                      'brc_stat_brc1',\n",
      "                      'vehspd_cval_cpc',\n",
      "                      'vehweight_cval_pt',\n",
      "                      'actualspeed_pti1',\n",
      "                      'selgr_rq_pt',\n",
      "                      'rmsmotorcurrent_pti1',\n",
      "                      'roadgrad_cval_pt',\n",
      "                      'currpwr_contendrnbrkresist_cval',\n",
      "                      'elcomp_pwrcons_cval',\n",
      "                      'hv_ptc_cabin1_pwr_cval',\n",
      "                      'maxrecuppwrprc_cval',\n",
      "                      'txoiltemp_cval_tcm',\n",
      "                      'start_soc',\n",
      "                      'hv_curr_cval_dcl1',\n",
      "                      'hv_dclink_volt_cval_dcl1',\n",
      "                      'hv_pwr_cval_dcl1',\n",
      "                      'lv_convpwr_cval_dcl1',\n",
      "                      'hv_batmaxchrgpwrlim_cval_1']\n",
      "11  TARGETS          ['hv_bat_soc_cval_bms1']\n",
      "12  PRIORS           ['emot_soc_pred']\n",
      "13  HIDDEN_SIZE      512\n",
      "14  NUM_LAYERS       3\n",
      "15  DROPOUT          0.5\n",
      "16  NUM_EPOCHS       4\n",
      "17  BATCH_SIZE       32\n",
      "18  LEARNING_RATE    0.002\n",
      "19  OPTIMIZER        torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = 1e-3)\n",
      "20  LRSCHEDULER      torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 3, factor = 0.5, min_lr = 1e-7)\n",
      "21  CRITERION        nn.SmoothL1Loss()\n",
      "22  LOSS_FN          F.mse_loss(output, target)\n",
      "23  MODE             train_mode\n",
      "24  TRAIN_LOG        test2.txt \n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Torch version:  2.4.1+cu121\n",
      "Using: -->  CUDA:0\n"
     ]
    }
   ],
   "source": [
    "# LOCATE REPOSITORY/DATASTORAGE IN CURRENT SYSTEM ENVIRONMENT  ---------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "import sys, os\n",
    "for key in CONFIG: globals()[key] = CONFIG[key]\n",
    "if 'ROOT' not in globals(): ROOT = Path('../..').resolve()\n",
    "sys.path.append(os.path.abspath(ROOT))\n",
    "\n",
    "# INTERNAL MODULE IMPORTS ----------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------\n",
    "from src.__init__ import *\n",
    "from src.utils.data_utils import *\n",
    "from src.utils.preprocess_utils import *\n",
    "from src.utils.eval_utils import *\n",
    "from src.utils.Trainers import *\n",
    "#from src.LSTM.lstm_models import *\n",
    "\n",
    "# SETUP ENVIRONMENT ---------------------------------------------------------------------\n",
    "DATA_PATH, IS_NOTEBOOK, DEVICE = setup_environment(CONFIG, ROOT, SEED, GPU_SELECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Total Files:\t16116\n",
      "------------------------------------------------------------\n",
      "Obtaining sequence lengths... (may take up to 5 minutes)\n",
      "Filtered Files:\t3721\n",
      "------------------------------------------------------------\n",
      "               FileName  Length  Index\n",
      "0       V13_T25.parquet   20843    471\n",
      "1      V18_T775.parquet   19425   1393\n",
      "2      V13_T352.parquet   18308    456\n",
      "3      V18_T972.parquet   17858   1871\n",
      "4     V16_T1629.parquet   17519    916\n",
      "...                 ...     ...    ...\n",
      "3716   V18_T953.parquet    3612   2804\n",
      "3717   V12_T252.parquet    3609    949\n",
      "3718   V13_T381.parquet    3609   3325\n",
      "3719    V4_T280.parquet    3606   1355\n",
      "3720  V16_T1545.parquet    3606   2252\n",
      "\n",
      "[3721 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# FILE SOURCES ---------------------------------------------------------------\n",
    "input_folder = Path(DATA_PATH, INPUT_LOCATION) # Trip parquet files\n",
    "pth_folder = Path(ROOT, OUTPUT_LOCATION)\n",
    "files, trip_lengths, indices_by_length, sorted_trip_lengths, all_signals = prepare_data(input_folder, pth_folder, MAX_FILES, MIN_SEQ_LENGTH, ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT & TARGET SPECIFICATION ---------------------------------------------------\n",
    "# these signals are required for the physical Model calculation:\n",
    "base_signals = [\"signal_time\", \"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \n",
    "                \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\",\"bs_roadincln_cval\", \"roadgrad_cval_pt\"]\n",
    "\n",
    "# these signals have to be dropped (from Features) in order for appropriate training:\n",
    "columns_to_drop = [\"signal_time\",                       # works as index\n",
    "                    \"hirestotalvehdist_cval_icuc\",      # starts from 0, obtained by speed integration\n",
    "                    \"latitude_cval_ippc\",               # only GPS \n",
    "                    \"longitude_cval_ippc\",              # only GPS\n",
    "                    \"hv_batpwr_cval_bms1\",              # directly related to target (soc_gradient)\n",
    "                    \"hv_batmomavldischrgen_cval_1\",     # indirect target 1 in kWh\n",
    "                    \"hv_bat_soc_cval_bms1\",              # indirect target 2 in %SoC\n",
    "                    \"soc_gradient\",                     # actual target signal   \n",
    "                    \"emot_pwr_cval\",                    # replaced as physical prior for PINN\n",
    "                    \"emot_pwr_pred\",                    # actual physical prior for PINN\n",
    "                    ]\n",
    "\n",
    "# Ensure no element of \"columns_to_drop\" is included in \"FEATURES\"\n",
    "assert not any(col in FEATURES for col in columns_to_drop), \"Some columns to drop are still in FEATURES\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "selection_1 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", 'roadgrad_cval_pt', \"vehweight_cval_pt\", \"accelpdlposn_cval\", \"bs_brk_cval\", \"elcomp_pwrcons_cval\",\n",
    "               \"epto_pwr_cval\", \"motortemperature_pti1\", \"powerstagetemperature_pti1\", 'airtempinsd_cval_hvac', 'brktempra_cval', 'selgr_rq_pt']\n",
    "selection_2 = [\"hirestotalvehdist_cval_icuc\", \"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"hv_batpwr_cval_bms1\", \"emot_pwr_cval\", \"roadgrad_cval_pt\"]\n",
    "selection_3 = [\"vehspd_cval_cpc\", \"altitude_cval_ippc\", \"airtempoutsd_cval_cpc\", \"vehweight_cval_pt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input Signals:\t34\n",
      "Target Signals:\t1\n",
      "Physical Prior Signals:\t1\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# FEATURE SELECTION  ----------------------------------------------------------------------------\n",
    "INPUT_COLUMNS = FEATURES\n",
    "TARGET_COLUMN = TARGETS\n",
    "PRIOR_COLUMN = PRIORS\n",
    "print(f\"{'-'*60}\\nInput Signals:\\t{len(FEATURES)}\\nTarget Signals:\\t{len(TARGETS)}\\nPhysical Prior Signals:\\t{len(PRIORS)}\\n{'-'*60}\")\n",
    "\n",
    "# FEATURE NORMALIZATION/SCALING -----------------------------------------------------------------\n",
    "scaler = eval(SCALERS['feature_scaler'])\n",
    "target_scaler = eval(SCALERS['target_scaler'])\n",
    "prior_scaler = eval(SCALERS['prior_scaler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['actdrvtrnpwrprc_cval',\n",
       "  'altitude_cval_ippc',\n",
       "  'motortemperature_pti1',\n",
       "  'brktempra_cval',\n",
       "  'hv_batmaxdischrgpwrlim_cval_1',\n",
       "  'actualtorque_pti1',\n",
       "  'powerstagetemperature_pti1',\n",
       "  'accelpdlposn_cval',\n",
       "  'airtempoutsd_cval_cpc',\n",
       "  'airtempinsd_cval_hvac',\n",
       "  'actualdcvoltage_pti1',\n",
       "  'hv_batavcelltemp_cval_bms1',\n",
       "  'epto_pwr_cval',\n",
       "  'maxtracpwrpct_cval',\n",
       "  'airtempinsd_rq',\n",
       "  'bs_brk_cval',\n",
       "  'brc_stat_brc1',\n",
       "  'vehspd_cval_cpc',\n",
       "  'vehweight_cval_pt',\n",
       "  'actualspeed_pti1',\n",
       "  'selgr_rq_pt',\n",
       "  'rmsmotorcurrent_pti1',\n",
       "  'roadgrad_cval_pt',\n",
       "  'currpwr_contendrnbrkresist_cval',\n",
       "  'elcomp_pwrcons_cval',\n",
       "  'hv_ptc_cabin1_pwr_cval',\n",
       "  'maxrecuppwrprc_cval',\n",
       "  'txoiltemp_cval_tcm',\n",
       "  'start_soc',\n",
       "  'hv_curr_cval_dcl1',\n",
       "  'hv_dclink_volt_cval_dcl1',\n",
       "  'hv_pwr_cval_dcl1',\n",
       "  'lv_convpwr_cval_dcl1',\n",
       "  'hv_batmaxchrgpwrlim_cval_1'],\n",
       " ['hv_bat_soc_cval_bms1'],\n",
       " ['emot_soc_pred'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Warning: Removed the last 1 samples to ensure a balanced batch size\n",
      "fitting Scalers: MaxAbsScaler, MinMaxScaler, MinMaxScaler\n",
      "\t50% of the fitting done...\n",
      "Done. Create DataSets and DataLoaders...\n",
      "\tNumber of batches created: 93\n",
      "\tNumber of batches created: 21\n",
      "\tNumber of batches created: 3\n",
      "------------------------------------------------------------\n",
      "Train size:  21553734\t\t(Files: 2976)\n",
      "Val. size:   4864710\t\t(Files: 670)\n",
      "Test size:   570266\t\t(Files: 74) \n",
      " ------------------------------------------------------------\n",
      "\tRemoved 1 file from the dataset\n",
      "------------------------------------------------------------\n",
      "first 3 train files: ['V13_T25.parquet', 'V18_T775.parquet', 'V13_T352.parquet']\n"
     ]
    }
   ],
   "source": [
    "# GENERATE DATALOADERS ---------------------------------------------------------------\n",
    "\n",
    "# DATA SET SPLITTING AND SORTING ----------------------------------------------------------------\n",
    "train_subset, val_subset, test_subset = random_split(files, TRAIN_VAL_TEST)\n",
    "\n",
    "# DATALOADER SETTINGS ------------------------------------------------------------------\n",
    "dataloader_settings = {\n",
    "    'batch_size': 1,                    # see *Note above\n",
    "    'shuffle': True,                    # shuffle the batches before each epoch\n",
    "    'collate_fn': collate_fn_PINN,      # include optional arguments\n",
    "    'num_workers': 4,                   # number of workers\n",
    "    'pin_memory': False if DEVICE.type == 'cpu' else True}\n",
    "\n",
    "# PREPARE TRAIN, VAL & TEST DATALOADERS  ------------------------------------------------------------\n",
    "train_subset, train_dataset, train_dataset_batches, train_loader = prepare_dataloader_PINN(train_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings, fit=True, drop_last=True)\n",
    "\n",
    "val_subset, val_dataset, val_dataset_batches, val_loader = prepare_dataloader_PINN(val_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings)\n",
    "\n",
    "test_subset, test_dataset, test_dataset_batches, test_loader = prepare_dataloader_PINN(test_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, PRIOR_COLUMN, scaler, target_scaler, prior_scaler, dataloader_settings)\n",
    "\n",
    "'''\n",
    "# PREPARE TRAIN, VAL & TEST DATALOADERS  ------------------------------------------------------------\n",
    "train_subset, train_dataset, train_dataset_batches, train_loader = prepare_dataloader(train_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, scaler, target_scaler, dataloader_settings, fit=True)\n",
    "\n",
    "val_subset, val_dataset, val_dataset_batches, val_loader = prepare_dataloader(val_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, scaler, target_scaler, dataloader_settings, fit=False)\n",
    "\n",
    "test_subset, test_dataset, test_dataset_batches, test_loader = prepare_dataloader(test_subset, indices_by_length, \\\n",
    "    BATCH_SIZE, INPUT_COLUMNS, TARGET_COLUMN, scaler, target_scaler, dataloader_settings, fit=False)\n",
    "'''\n",
    "# print dataset info\n",
    "subset_files = print_dataset_sizes(train_dataset, val_dataset, test_dataset, train_subset, val_subset, test_subset, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM NETWORK -----------------------------------------------------------------------\n",
    "\n",
    "class LSTM1_packed_old_version(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, device=DEVICE):\n",
    "        super(LSTM1_packed_old_version, self).__init__()\n",
    "\n",
    "        self.input_size = input_size    # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.num_layers = num_layers    # number of layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # LSTM CELL --------------------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size,            # The number of expected features in the input x\n",
    "            self.hidden_size,           # The number of features in the hidden state h\n",
    "            self.num_layers,            # Number of recurrent layers for stacked LSTMs. Default: 1\n",
    "            batch_first=True,           # If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Default: False\n",
    "            bias=True,                  # If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            dropout=self.dropout,       # usually: [0.2 - 0.5], introduces a Dropout layer on the outputs of each LSTM layer except the last layer, (dropout probability). Default: 0\n",
    "            bidirectional=False,        # If True, becomes a bidirectional LSTM. Default: False\n",
    "            proj_size=0,                # If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "            device=device)\n",
    "\n",
    "        # LAYERS -----------------------------------\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 4)\n",
    "        self.fc3 = nn.Linear(hidden_size // 4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, packed_input, batch_size=None):\n",
    "        packed_out, _ = self.lstm(packed_input)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.dropout_layer(out)  # dropout\n",
    "        out = self.fc1(out)  # fully connected layer 1\n",
    "        out = self.bn1(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc2(out)  # fully connected layer 2\n",
    "        out = self.bn2(out.transpose(1, 2)).transpose(1, 2)  # batch normalization 2\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc3(out)  # fully connected layer 3\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " LSTM1_packed_old_version(\n",
      "  (lstm): LSTM(34, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ") ------------------------------------------------------------\n",
      "Model state_dict:\n",
      "lstm.weight_ih_l0:\t torch.Size([2048, 34])\n",
      "lstm.weight_hh_l0:\t torch.Size([2048, 512])\n",
      "lstm.bias_ih_l0:\t torch.Size([2048])\n",
      "lstm.bias_hh_l0:\t torch.Size([2048])\n",
      "lstm.weight_ih_l1:\t torch.Size([2048, 512])\n",
      "lstm.weight_hh_l1:\t torch.Size([2048, 512])\n",
      "lstm.bias_ih_l1:\t torch.Size([2048])\n",
      "lstm.bias_hh_l1:\t torch.Size([2048])\n",
      "lstm.weight_ih_l2:\t torch.Size([2048, 512])\n",
      "lstm.weight_hh_l2:\t torch.Size([2048, 512])\n",
      "lstm.bias_ih_l2:\t torch.Size([2048])\n",
      "lstm.bias_hh_l2:\t torch.Size([2048])\n",
      "fc1.weight:\t torch.Size([256, 512])\n",
      "fc1.bias:\t torch.Size([256])\n",
      "bn1.weight:\t torch.Size([256])\n",
      "bn1.bias:\t torch.Size([256])\n",
      "bn1.running_mean:\t torch.Size([256])\n",
      "bn1.running_var:\t torch.Size([256])\n",
      "bn1.num_batches_tracked:\t torch.Size([])\n",
      "fc2.weight:\t torch.Size([128, 256])\n",
      "fc2.bias:\t torch.Size([128])\n",
      "bn2.weight:\t torch.Size([128])\n",
      "bn2.bias:\t torch.Size([128])\n",
      "bn2.running_mean:\t torch.Size([128])\n",
      "bn2.running_var:\t torch.Size([128])\n",
      "bn2.num_batches_tracked:\t torch.Size([])\n",
      "fc3.weight:\t torch.Size([1, 128])\n",
      "fc3.bias:\t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# MODEL CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# INSTANTIATE MODEL --------------------\n",
    "model = LSTM1_packed_old_version(len(INPUT_COLUMNS), HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "print(f\"{'-'*60}\\n\", model, f\"{'-'*60}\\nModel state_dict:\")\n",
    "for param_tensor in model.state_dict(): print(f\"{param_tensor}:\\t {model.state_dict()[param_tensor].size()}\") \n",
    "# --> Note torch.Size([4*hidden_size, input_size]) for LSTM weights because of i,o,f,g params concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Shape of packed_inputs.data: torch.Size([153290, 34])\n",
      "Lengths: tensor([4849, 4847, 4829, 4821, 4819, 4812, 4811, 4806, 4806, 4804, 4803, 4796,\n",
      "        4795, 4791, 4788, 4788, 4788, 4786, 4784, 4779, 4777, 4777, 4775, 4774,\n",
      "        4774, 4772, 4771, 4767, 4751, 4751, 4750, 4749])\n"
     ]
    }
   ],
   "source": [
    "if IS_NOTEBOOK and True: \n",
    "    check_batch_PINN(train_loader)\n",
    "    #visualize_padding(BATCH_SIZE, trip_lengths, sorted_trip_lengths, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.002\n",
      "    maximize: False\n",
      "    weight_decay: 0.001\n",
      ")\n",
      "------------------------------------------------------------\n",
      "LRScheduler: <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TRAINING CONFIGURATION -----------------------------------------------------------------------\n",
    "\n",
    "# OPTIMIZER --------------------------------------------------------------------------------\n",
    "# common optimizers: ['torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.RMSprop']\n",
    "if 'OPTIMIZER' in globals(): optimizer = eval(OPTIMIZER)\n",
    "else: optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE,\n",
    "        weight_decay = 1e-4      # weight decay coefficient (default: 1e-2)\n",
    "        #betas = (0.9, 0.95),    # coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        #eps = 1e-8,             # term added to the denominator to improve numerical stability (default: 1e-8)\n",
    ")\n",
    "print(f\"{'-'*60}\\n{optimizer}\\n{'-'*60}\")\n",
    "\n",
    "# LR SCHEDULER ----------------------------------------------------------------------------\n",
    "if 'LRSCHEDULER' in globals(): scheduler = eval(LRSCHEDULER)\n",
    "else: scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 2, factor = 0.5, min_lr = 1e-6)\n",
    "print(f\"LRScheduler: {scheduler.__class__}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Fn: F.mse_loss(output, target)\n",
      "------------------------------------------------------------\n",
      " Criterion: <class 'torch.nn.modules.loss.SmoothL1Loss'>\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOSS FUNCTION ----------------------------------------------------------------\n",
    "def loss_fn(output, target):\n",
    "    if 'LOSS_FN' in globals(): loss = eval(LOSS_FN)\n",
    "    else: loss = F.mse_loss(output, target) # mean-squared error for regression\n",
    "    return loss\n",
    "\n",
    "def loss_fn_PINN_1(output, target, prior):\n",
    "    loss = F.mse_loss(output, target, reduction='mean') # mean-squared error for regression\n",
    "    return loss\n",
    "\n",
    "# or define criterion function:\n",
    "criterion_list = [nn.MSELoss(), nn.L1Loss(), nn.SmoothL1Loss(), nn.HuberLoss(), MASE()]\n",
    "\n",
    "if 'CRITERION' in globals(): criterion = eval(CRITERION)\n",
    "else: criterion = nn.SmoothL1Loss()\n",
    "print(f\"Loss_Fn: {LOSS_FN}\\n{'-'*60}\\n\", f\"Criterion: {criterion.__class__}\\n{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_PINN_2(output, target, prior):\n",
    "    y_pred = output\n",
    "    y_true = target\n",
    "    y_phys = prior\n",
    "\n",
    "    mse_loss = F.mse_loss(y_pred, y_true)\n",
    "    phys_loss = F.mse_loss(y_pred, y_phys)\n",
    "\n",
    "    total_loss = mse_loss + phys_loss\n",
    "    return total_loss\n",
    "\n",
    "def loss_fn_PINN_3(output, target, prior):\n",
    "    l_p = 0.5\n",
    "\n",
    "    y_pred = output\n",
    "    y_true = target\n",
    "    y_phys = prior\n",
    "\n",
    "    total_loss = F.mse_loss(y_true, (l_p * y_phys + (1 - l_p) * y_pred))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET NETWORK TRAINER -----------------------------------------------------------------\n",
    "TRAINER = PTrainer_PINN(  #PTrainer_Standard, PTrainer_PINN\n",
    "    model = model, \n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler, \n",
    "    loss_fn = loss_fn_PINN_1, \n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    "    num_epochs = NUM_EPOCHS, \n",
    "    device = DEVICE, \n",
    "    is_notebook = IS_NOTEBOOK,\n",
    "    use_mixed_precision = False,\n",
    "    log_file = TRAIN_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Training Started.\tProcess ID: 822917 \n",
      "------------------------------------------------------------\n",
      "Model: LSTM1_packed_old_version\tParameters on device: CUDA:0\n",
      "------------------------------------------------------------\n",
      "Train/Batch size:\t93 / 1\n",
      "Loss:\t\t\t<function loss_fn_PINN_1 at 0x7f1ab8f95bc0>\n",
      "Optimizer:\t\tAdamW\n",
      "LR:\t\t\t0.002\n",
      "Weight Decay:\t\t0.001\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"scrollable_table\" style=\"height: 300px; overflow-y: scroll;\">\n",
       "    <table id=\"training_table\" style=\"width:60%; border-collapse: collapse;\">\n",
       "        <thead style=\"position: sticky; top: 0; z-index: 1;\">\n",
       "            <tr>\n",
       "                <th style=\"font-weight:bold; width:15%; text-align:left; padding: 10px; background-color: #404040;\">Epoch</th>\n",
       "                <th style=\"font-weight:bold; width:25%; text-align:left; padding: 10px; background-color: #404040;\">Iteration</th>\n",
       "                <th style=\"font-weight:bold; width:30%; text-align:left; padding: 10px; background-color: #404040;\">Batch Loss</th>\n",
       "                <th style=\"font-weight:bold; width:30%; text-align:left; padding: 10px; background-color: #404040;\">Train Loss</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        </tbody>\n",
       "    </table>\n",
       "    <script>\n",
       "        function addRow(epoch, step, loss, running_loss) {\n",
       "            var table = document.getElementById(\"training_table\").getElementsByTagName('tbody')[0];\n",
       "            var row = table.insertRow(-1);\n",
       "            var cell1 = row.insertCell(0);\n",
       "            var cell2 = row.insertCell(1);\n",
       "            var cell3 = row.insertCell(2);\n",
       "            var cell4 = row.insertCell(3);\n",
       "            cell1.style.textAlign = \"left\";\n",
       "            cell2.style.textAlign = \"left\";\n",
       "            cell3.style.textAlign = \"left\";\n",
       "            cell4.style.textAlign = \"left\";\n",
       "            cell1.innerHTML = epoch;\n",
       "            cell2.innerHTML = step;\n",
       "            cell3.innerHTML = loss;\n",
       "            cell4.innerHTML = running_loss;\n",
       "            var scrollableDiv = document.getElementById(\"scrollable_table\");\n",
       "            scrollableDiv.scrollTop = scrollableDiv.scrollHeight;\n",
       "        }\n",
       "    </script>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1499fded5e4c4ca2aaa2c25f0124e2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>addRow(\"<b>1/4\", \"1/93\", \"0.204225\", \"\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# START TRAINING -----------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_mode\u001b[39m\u001b[38;5;124m'\u001b[39m: CHECKPOINT \u001b[38;5;241m=\u001b[39m TRAINER\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "File \u001b[0;32m~/MA-eR-PINN/src/utils/Trainers.py:220\u001b[0m, in \u001b[0;36mPTrainer_PINN.train_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# -------------------------------------\u001b[39;00m\n\u001b[1;32m    219\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn_pinn(outputs\u001b[38;5;241m.\u001b[39msqueeze(), targets, priors)\n\u001b[0;32m--> 220\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), clip_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value)  \u001b[38;5;66;03m# optional: Gradient Value Clipping\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# START TRAINING -----------------------------------------------------------------\n",
    "if MODE == 'train_mode': CHECKPOINT = TRAINER.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "CHECKPOINT, model_destination_path = save_checkpoint(TRAINER, train_loader, val_loader, test_loader, CHECKPOINT, CONFIG, subset_files, pth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip\n",
    "# LOAD MODEL -----------------------------------------------------------------\n",
    "#model_destination_path = Path(pth_folder, \"LSTM1_241211_225933.pth\")\n",
    "CHECKPOINT = load_checkpoint(model_destination_path, DEVICE)\n",
    "\n",
    "# get model type:\n",
    "for key in CHECKPOINT.keys(): globals()[key] = CHECKPOINT[key]\n",
    "\n",
    "# load model and optimizer states\n",
    "model.load_state_dict(model_state_dict)\n",
    "optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "model.eval()  # set model to evaluation mode for inference\n",
    "print(f\"Model loaded from:\\t{model_destination_path}\\n{'-'*60}\\nModel: {model.__class__.__name__}\\tParameters on device: {next(model.parameters()).device}\"\n",
    "        f\"\\n{'-'*60}\\nTrain/Batch size:\\t{len(train_loader.dataset)} / {train_loader.batch_size}\\n\"\n",
    "        f\"Loss:\\t\\t\\t{loss_fn}\\nOptimizer:\\t\\t{optimizer.__class__.__name__}\\nLR:\\t\\t\\t\"\n",
    "        f\"{optimizer.param_groups[0]['lr']}\\nWeight Decay:\\t\\t{optimizer.param_groups[0]['weight_decay']}\\n{'-'*60}\\n\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING PERFORMANCE -----------------------------------------------------------------\n",
    "# get DataFrame of training metrics:\n",
    "keys = ['training_table', 'train_losses_per_iter', 'train_losses', 'val_losses', 'lr_history', 'train_batches']\n",
    "training_table, train_losses_per_iter, train_losses, val_losses, lr_history, train_batches = (CHECKPOINT[key] for key in keys)\n",
    "training_df = pd.DataFrame(training_table, columns=[\"Epoch\", \"Iteration\", \"Batch Loss\", \"Train Loss\"])\n",
    "\n",
    "# -------------------------------------\n",
    "NUM_EPOCHS = CONFIG['NUM_EPOCHS']\n",
    "plot_training_performance(training_df, train_losses_per_iter, train_losses, val_losses, lr_history, train_batches, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_outputs_targets_priors(outputs, targets, priors, original_lengths) -> tuple:\n",
    "    all_outputs, all_targets, all_priors, all_original_lengths = [], [], [], []\n",
    "    for batch_outputs, batch_targets, batch_priors, batch_lengths in zip(outputs, targets, priors, original_lengths):\n",
    "        all_outputs.extend(batch_outputs)\n",
    "        all_targets.extend(batch_targets)\n",
    "        all_priors.extend(batch_priors)\n",
    "        all_original_lengths.extend(batch_lengths)\n",
    "        return all_outputs, all_targets, all_priors, all_original_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION -----------------------------------------------------------------\n",
    "\n",
    "# get file list of test subset\n",
    "test_files = CHECKPOINT[\"test_files\"]\n",
    "# -------------------------------------\n",
    "# evaluate model on test set\n",
    "test_loss, outputs, targets, priors, original_lengths = TRAINER.evaluate_model()\n",
    "# -------------------------------------\n",
    "all_outputs, all_targets, all_priors, all_original_lengths = concat_outputs_targets_priors(outputs, targets, priors, original_lengths)\n",
    "\n",
    "# Inverse-transform on all outputs and targets for evaluation\n",
    "scaled_outputs = [target_scaler.inverse_transform(output_sequence.reshape(1, -1)).squeeze() for output_sequence in all_outputs]\n",
    "scaled_targets = [target_scaler.inverse_transform(target_sequence.reshape(1, -1)).squeeze() for target_sequence in all_targets]\n",
    "\n",
    "all_y_true,all_y_pred  = np.concatenate(scaled_targets), np.concatenate(scaled_outputs)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "rmse = root_mean_squared_error(all_y_true, all_y_pred)\n",
    "avg_error = np.mean(np.abs(all_y_true - all_y_pred))\n",
    "std_dev = np.std(all_y_true - all_y_pred)\n",
    "mape = np.mean(np.abs((all_y_true - all_y_pred) / all_y_true)) * 100\n",
    "\n",
    "print(f\"Test Loss:  {test_loss:.6f}\\nRMSE: {rmse:.4f}\\nStandard Deviation: {std_dev:.4f}\\nAvg Err. ± STD: {avg_error:.4f} ± {std_dev:.4f} ({mape:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT RESULTS -----------------------------------------------------------------\n",
    "# get random sample sequence from test set\n",
    "# -------------------------------------\n",
    "sample_int = random.randint(1, len(scaled_outputs)-1)\n",
    "y_pred = scaled_outputs[sample_int]\n",
    "y_true = scaled_targets[sample_int]\n",
    "\n",
    "# -------------------------------------\n",
    "def plot_prediction(y_true, y_pred, plot_active=True):\n",
    "     if plot_active:\n",
    "          plt.figure(figsize=(18,4))\n",
    "          plt.xlabel('Time in s')\n",
    "          plt.ylabel('SOC Change Rate in %/s')\n",
    "          plt.title('Battery State of Charge: Prediction vs. Actual Data')\n",
    "          plt.plot(y_true, label='Actual Data')  # actual plot\n",
    "          plt.plot(np.arange(0, len(y_true), 1), y_pred, label='Predicted Data')  # predicted plot\n",
    "          plt.legend()\n",
    "          plt.text(0.01, 0.02, f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\\nStd Dev: {np.std(y_true - y_pred):.4f}\",\n",
    "          transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "          plt.figure(figsize=(18,4))\n",
    "          plt.xlabel('Time in s')\n",
    "          plt.ylabel('SOC in %')\n",
    "          plt.plot(savgol_filter(y_true.flatten(), window_length=60, polyorder=3), label='Actual Data (Smoothed)')  # actual plot\n",
    "          plt.plot(np.arange(0, len(y_true), 1), savgol_filter(y_pred.flatten(), window_length=60, polyorder=3), label='Predicted Data (Smoothed)')  # predicted plot\n",
    "          plt.legend()\n",
    "\n",
    "          \n",
    "plot_prediction(y_true, y_pred, PLOT_ACTIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,4))\n",
    "plt.plot(np.cumsum(y_pred), label='SOC Predicted')\n",
    "plt.plot(np.cumsum(y_true), label='Actual SOC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
